{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## IMPORT ######################\n",
    "import json\n",
    "from datetime import datetime\n",
    "from functools import partial, wraps\n",
    "\n",
    "import fire\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, random, value_and_grad, vmap\n",
    "# from jax.experimental import optimizers\n",
    "from jax.example_libraries import optimizers\n",
    "from jax_md import space\n",
    "from shadow.plot import *\n",
    "from sklearn.metrics import r2_score\n",
    "from psystems.nsprings import (chain, edge_order, get_connections,\n",
    "                               get_fully_connected_senders_and_receivers,\n",
    "                               get_fully_edge_order)\n",
    "# from statistics import mode\n",
    "# from sympy import LM\n",
    "# from torch import batch_norm_gather_stats_with_counts\n",
    "import sys\n",
    "MAINPATH = \"..\"  # nopep8\n",
    "sys.path.append(MAINPATH)  # nopep8\n",
    "import jraph\n",
    "import src\n",
    "from jax.config import config\n",
    "# from src import fgn, lnn\n",
    "from src.graph import *\n",
    "# from src.lnn import acceleration, accelerationFull, accelerationTV\n",
    "from src.md import *\n",
    "from src.models import MSE, initialize_mlp, GaussianNLL, initialize_mlp_gamma, forward_pass_gamma\n",
    "from src.nve import NVEStates, nve, BrownianStates\n",
    "from src.utils import *\n",
    "\n",
    "# config.update(\"jax_enable_x64\", True)\n",
    "# config.update(\"jax_debug_nans\", True)\n",
    "# jax.config.update('jax_platform_name', 'gpu')\n",
    "\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "\n",
    "def pprint(*args, namespace=globals()):\n",
    "    for arg in args:\n",
    "        print(f\"{namestr(arg, namespace)[0]}: {arg}\")\n",
    "\n",
    "f32 = jnp.float32\n",
    "f64 = jnp.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs: \n",
      "N: 10\n",
      "epochs: 10000\n",
      "seed: 42\n",
      "rname: True\n",
      "dt: 0.001\n",
      "lr: 0.0001\n",
      "batch_size: 20\n"
     ]
    }
   ],
   "source": [
    "N = 10  # number of particles\n",
    "dim = 2  # dimensions\n",
    "runs = 1\n",
    "kT = 1 #1.380649e-23*T  # boltzmann constant*temperature\n",
    "# spring_constant = 1.0\n",
    "# length_constant = 1.0\n",
    "# nconfig=100\n",
    "seed=42\n",
    "dt = 1.0e-3 # time step*stride \n",
    "lr=1e-4\n",
    "batch_size=20\n",
    "epochs = 10000\n",
    "# node_type = jnp.array([0,0,0,0,0])\n",
    "masses = jnp.ones(N)\n",
    "# masses = 1.0#jnp.where(jnp.arange(N) <3, 1.0, 1.0)\n",
    "species = np.where(np.arange(N) < 3, 0, 1) #jnp.zeros(N, dtype=int)\n",
    "# gamma = jnp.where(jnp.arange(N) <3, 1.0, 2.0).reshape(-1,1)\n",
    "\n",
    "rname=True\n",
    "withdata = None\n",
    "\n",
    "print(\"Configs: \")\n",
    "pprint(N, epochs, seed, rname, dt, lr, batch_size, namespace=locals())\n",
    "\n",
    "randfilename = datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "\n",
    "PSYS = f\"a-{N}-AB-Spring-data-brownian_EM\"\n",
    "TAG = f\"1NN\"\n",
    "out_dir = f\"../results\"\n",
    "\n",
    "def _filename(name, tag=TAG):\n",
    "    rstring = randfilename if (rname and (tag != \"data\")) else (\n",
    "        \"0\" if (tag == \"data\") or (withdata == None) else f\"0_{withdata}\")\n",
    "    filename_prefix = f\"{out_dir}/{PSYS}-{tag}/{rstring}/\"\n",
    "    file = f\"{filename_prefix}/{name}\"\n",
    "    os.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "    filename = f\"{filename_prefix}/{name}\".replace(\"//\", \"/\")\n",
    "    print(\"===\", filename, \"===\")\n",
    "    return filename\n",
    "\n",
    "def displacement(a, b):\n",
    "    return a - b\n",
    "\n",
    "def shift(R, dR):\n",
    "    return R+dR\n",
    "\n",
    "def OUT(f):\n",
    "    @wraps(f)\n",
    "    def func(file, *args, tag=TAG, **kwargs):\n",
    "        return f(_filename(file, tag=tag), *args, **kwargs)\n",
    "    return func\n",
    "\n",
    "loadmodel = OUT(src.models.loadmodel)\n",
    "savemodel = OUT(src.models.savemodel)\n",
    "\n",
    "loadfile = OUT(src.io.loadfile)\n",
    "savefile = OUT(src.io.savefile)\n",
    "save_ovito = OUT(src.io.save_ovito)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ../results/a-10-AB-Spring-data-brownian_EM-data/0/model_states_brownian.pkl ===\n",
      "Total number of data points: 100x100\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "################## CONFIG ######################\n",
    "################################################\n",
    "np.random.seed(seed)\n",
    "key = random.PRNGKey(seed)\n",
    "\n",
    "try:\n",
    "    dataset_states = loadfile(f\"model_states_brownian.pkl\", tag=\"data\")[0]\n",
    "except:\n",
    "    raise Exception(\"Generate dataset first.\")\n",
    "\n",
    "model_states = dataset_states[0]\n",
    "\n",
    "print(f\"Total number of data points: {len(dataset_states)}x{model_states.position.shape[0]}\")\n",
    "\n",
    "Rs = States_Brow().fromlist(dataset_states).get_array()\n",
    "\n",
    "Rs_in = Rs[:,:99,:,:]\n",
    "Rs_out = Rs[:,1:100,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "################### ML Model ###################\n",
    "################################################\n",
    "# print(\"Creating Chain\")\n",
    "x, _, senders, receivers = chain(N)\n",
    "\n",
    "hidden = 16\n",
    "nhidden = 2\n",
    "\n",
    "def get_layers(in_, out_):\n",
    "    return [in_] + [hidden]*nhidden + [out_]\n",
    "\n",
    "def mlp(in_, out_, key, **kwargs):\n",
    "    return initialize_mlp(get_layers(in_, out_), key, **kwargs)\n",
    "\n",
    "params = {\"F_pos\": mlp(N*dim, N*dim, key)}\n",
    "\n",
    "def acceleration_node(x, params, **kwargs):\n",
    "    n,dim = x.shape\n",
    "    inp = x.flatten() #jnp.hstack([x.flatten(),v.flatten()])\n",
    "    out = forward_pass(params, inp)\n",
    "    return out.reshape(-1,dim)\n",
    "\n",
    "def _force_fn():    \n",
    "    def apply(R, params):\n",
    "        return acceleration_node(R, params)\n",
    "    return apply\n",
    "\n",
    "apply_fn = _force_fn()\n",
    "\n",
    "def next_step_fn_model(x, params): return apply_fn(x, params['F_pos'])\n",
    "v_next_step_fn_model = vmap(next_step_fn_model, in_axes=(0, None))\n",
    "v_v_next_step_fn_model = vmap(v_next_step_fn_model, in_axes=(0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.3148115 , -0.63787794],\n",
       "       [ 0.402592  , -1.4643216 ],\n",
       "       [ 1.6488845 ,  0.83136076],\n",
       "       [-0.00761402,  1.18472   ],\n",
       "       [-1.2101454 , -3.1200657 ],\n",
       "       [ 0.78753513,  1.0262994 ],\n",
       "       [ 2.007229  , -2.381413  ],\n",
       "       [-1.4420643 , -2.0012522 ],\n",
       "       [-0.22541031, -5.352497  ],\n",
       "       [-0.868411  , -2.5724523 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_step_fn_model(x, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ...\n",
      "Epoch: 0/10000 Loss (MSE):  train=7.16481351852417\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 10/10000 Loss (MSE):  train=6.222112655639648\n",
      "Epoch: 20/10000 Loss (MSE):  train=5.456435680389404\n",
      "Epoch: 30/10000 Loss (MSE):  train=4.830929756164551\n",
      "Epoch: 40/10000 Loss (MSE):  train=4.310903549194336\n",
      "Epoch: 50/10000 Loss (MSE):  train=3.8714799880981445\n",
      "Epoch: 60/10000 Loss (MSE):  train=3.4948155879974365\n",
      "Epoch: 70/10000 Loss (MSE):  train=3.167975902557373\n",
      "Epoch: 80/10000 Loss (MSE):  train=2.881399631500244\n",
      "Epoch: 90/10000 Loss (MSE):  train=2.6278786659240723\n",
      "Epoch: 100/10000 Loss (MSE):  train=2.4018778800964355\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 110/10000 Loss (MSE):  train=2.199073076248169\n",
      "Epoch: 120/10000 Loss (MSE):  train=2.0160601139068604\n",
      "Epoch: 130/10000 Loss (MSE):  train=1.8501489162445068\n",
      "Epoch: 140/10000 Loss (MSE):  train=1.6991990804672241\n",
      "Epoch: 150/10000 Loss (MSE):  train=1.5615301132202148\n",
      "Epoch: 160/10000 Loss (MSE):  train=1.4358172416687012\n",
      "Epoch: 170/10000 Loss (MSE):  train=1.3210358619689941\n",
      "Epoch: 180/10000 Loss (MSE):  train=1.2163972854614258\n",
      "Epoch: 190/10000 Loss (MSE):  train=1.1212873458862305\n",
      "Epoch: 200/10000 Loss (MSE):  train=1.0352293252944946\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 210/10000 Loss (MSE):  train=0.9578235149383545\n",
      "Epoch: 220/10000 Loss (MSE):  train=0.8887038230895996\n",
      "Epoch: 230/10000 Loss (MSE):  train=0.8274930119514465\n",
      "Epoch: 240/10000 Loss (MSE):  train=0.7737690806388855\n",
      "Epoch: 250/10000 Loss (MSE):  train=0.7270413637161255\n",
      "Epoch: 260/10000 Loss (MSE):  train=0.6867472529411316\n",
      "Epoch: 270/10000 Loss (MSE):  train=0.6522620916366577\n",
      "Epoch: 280/10000 Loss (MSE):  train=0.6229214668273926\n",
      "Epoch: 290/10000 Loss (MSE):  train=0.5980531573295593\n",
      "Epoch: 300/10000 Loss (MSE):  train=0.5770058631896973\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 310/10000 Loss (MSE):  train=0.5591759085655212\n",
      "Epoch: 320/10000 Loss (MSE):  train=0.5440225601196289\n",
      "Epoch: 330/10000 Loss (MSE):  train=0.5310758352279663\n",
      "Epoch: 340/10000 Loss (MSE):  train=0.5199345350265503\n",
      "Epoch: 350/10000 Loss (MSE):  train=0.5102667808532715\n",
      "Epoch: 360/10000 Loss (MSE):  train=0.5018003582954407\n",
      "Epoch: 370/10000 Loss (MSE):  train=0.49431127309799194\n",
      "Epoch: 380/10000 Loss (MSE):  train=0.4876216948032379\n",
      "Epoch: 390/10000 Loss (MSE):  train=0.48158562183380127\n",
      "Epoch: 400/10000 Loss (MSE):  train=0.4760872721672058\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 410/10000 Loss (MSE):  train=0.47103211283683777\n",
      "Epoch: 420/10000 Loss (MSE):  train=0.4663461148738861\n",
      "Epoch: 430/10000 Loss (MSE):  train=0.4619680643081665\n",
      "Epoch: 440/10000 Loss (MSE):  train=0.45784837007522583\n",
      "Epoch: 450/10000 Loss (MSE):  train=0.45394790172576904\n",
      "Epoch: 460/10000 Loss (MSE):  train=0.4502335786819458\n",
      "Epoch: 470/10000 Loss (MSE):  train=0.4466802477836609\n",
      "Epoch: 480/10000 Loss (MSE):  train=0.4432658553123474\n",
      "Epoch: 490/10000 Loss (MSE):  train=0.43997257947921753\n",
      "Epoch: 500/10000 Loss (MSE):  train=0.43678751587867737\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 510/10000 Loss (MSE):  train=0.43369776010513306\n",
      "Epoch: 520/10000 Loss (MSE):  train=0.4306950569152832\n",
      "Epoch: 530/10000 Loss (MSE):  train=0.427770733833313\n",
      "Epoch: 540/10000 Loss (MSE):  train=0.4249189794063568\n",
      "Epoch: 550/10000 Loss (MSE):  train=0.4221343994140625\n",
      "Epoch: 560/10000 Loss (MSE):  train=0.41941124200820923\n",
      "Epoch: 570/10000 Loss (MSE):  train=0.41674700379371643\n",
      "Epoch: 580/10000 Loss (MSE):  train=0.41413918137550354\n",
      "Epoch: 590/10000 Loss (MSE):  train=0.41158342361450195\n",
      "Epoch: 600/10000 Loss (MSE):  train=0.40907877683639526\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 610/10000 Loss (MSE):  train=0.4066217541694641\n",
      "Epoch: 620/10000 Loss (MSE):  train=0.40421226620674133\n",
      "Epoch: 630/10000 Loss (MSE):  train=0.4018467366695404\n",
      "Epoch: 640/10000 Loss (MSE):  train=0.3995257019996643\n",
      "Epoch: 650/10000 Loss (MSE):  train=0.3972461521625519\n",
      "Epoch: 660/10000 Loss (MSE):  train=0.3950069546699524\n",
      "Epoch: 670/10000 Loss (MSE):  train=0.3928079605102539\n",
      "Epoch: 680/10000 Loss (MSE):  train=0.3906470537185669\n",
      "Epoch: 690/10000 Loss (MSE):  train=0.38852250576019287\n",
      "Epoch: 700/10000 Loss (MSE):  train=0.3864343464374542\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 710/10000 Loss (MSE):  train=0.38438090682029724\n",
      "Epoch: 720/10000 Loss (MSE):  train=0.3823614716529846\n",
      "Epoch: 730/10000 Loss (MSE):  train=0.38037416338920593\n",
      "Epoch: 740/10000 Loss (MSE):  train=0.3784182369709015\n",
      "Epoch: 750/10000 Loss (MSE):  train=0.37649279832839966\n",
      "Epoch: 760/10000 Loss (MSE):  train=0.3745962977409363\n",
      "Epoch: 770/10000 Loss (MSE):  train=0.37272822856903076\n",
      "Epoch: 780/10000 Loss (MSE):  train=0.3708873987197876\n",
      "Epoch: 790/10000 Loss (MSE):  train=0.36907199025154114\n",
      "Epoch: 800/10000 Loss (MSE):  train=0.36728188395500183\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 810/10000 Loss (MSE):  train=0.3655153810977936\n",
      "Epoch: 820/10000 Loss (MSE):  train=0.36377254128456116\n",
      "Epoch: 830/10000 Loss (MSE):  train=0.3620510399341583\n",
      "Epoch: 840/10000 Loss (MSE):  train=0.3603501319885254\n",
      "Epoch: 850/10000 Loss (MSE):  train=0.35866960883140564\n",
      "Epoch: 860/10000 Loss (MSE):  train=0.35700809955596924\n",
      "Epoch: 870/10000 Loss (MSE):  train=0.35536426305770874\n",
      "Epoch: 880/10000 Loss (MSE):  train=0.3537377715110779\n",
      "Epoch: 890/10000 Loss (MSE):  train=0.3521268963813782\n",
      "Epoch: 900/10000 Loss (MSE):  train=0.35053181648254395\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 910/10000 Loss (MSE):  train=0.3489508628845215\n",
      "Epoch: 920/10000 Loss (MSE):  train=0.34738314151763916\n",
      "Epoch: 930/10000 Loss (MSE):  train=0.34582847356796265\n",
      "Epoch: 940/10000 Loss (MSE):  train=0.3442853093147278\n",
      "Epoch: 950/10000 Loss (MSE):  train=0.3427528738975525\n",
      "Epoch: 960/10000 Loss (MSE):  train=0.34123048186302185\n",
      "Epoch: 970/10000 Loss (MSE):  train=0.3397172689437866\n",
      "Epoch: 980/10000 Loss (MSE):  train=0.33821216225624084\n",
      "Epoch: 990/10000 Loss (MSE):  train=0.33671510219573975\n",
      "Epoch: 1000/10000 Loss (MSE):  train=0.3352245092391968\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 1010/10000 Loss (MSE):  train=0.3337397873401642\n",
      "Epoch: 1020/10000 Loss (MSE):  train=0.33225974440574646\n",
      "Epoch: 1030/10000 Loss (MSE):  train=0.33078455924987793\n",
      "Epoch: 1040/10000 Loss (MSE):  train=0.3293130099773407\n",
      "Epoch: 1050/10000 Loss (MSE):  train=0.3278439939022064\n",
      "Epoch: 1060/10000 Loss (MSE):  train=0.32637733221054077\n",
      "Epoch: 1070/10000 Loss (MSE):  train=0.32491207122802734\n",
      "Epoch: 1080/10000 Loss (MSE):  train=0.3234473466873169\n",
      "Epoch: 1090/10000 Loss (MSE):  train=0.3219829797744751\n",
      "Epoch: 1100/10000 Loss (MSE):  train=0.3205186724662781\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 1110/10000 Loss (MSE):  train=0.319053053855896\n",
      "Epoch: 1120/10000 Loss (MSE):  train=0.31758660078048706\n",
      "Epoch: 1130/10000 Loss (MSE):  train=0.31611794233322144\n",
      "Epoch: 1140/10000 Loss (MSE):  train=0.3146481215953827\n",
      "Epoch: 1150/10000 Loss (MSE):  train=0.3131760358810425\n",
      "Epoch: 1160/10000 Loss (MSE):  train=0.3117014169692993\n",
      "Epoch: 1170/10000 Loss (MSE):  train=0.3102247416973114\n",
      "Epoch: 1180/10000 Loss (MSE):  train=0.30874550342559814\n",
      "Epoch: 1190/10000 Loss (MSE):  train=0.30726394057273865\n",
      "Epoch: 1200/10000 Loss (MSE):  train=0.305780291557312\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 1210/10000 Loss (MSE):  train=0.304294228553772\n",
      "Epoch: 1220/10000 Loss (MSE):  train=0.30280643701553345\n",
      "Epoch: 1230/10000 Loss (MSE):  train=0.301317036151886\n",
      "Epoch: 1240/10000 Loss (MSE):  train=0.2998259663581848\n",
      "Epoch: 1250/10000 Loss (MSE):  train=0.29833316802978516\n",
      "Epoch: 1260/10000 Loss (MSE):  train=0.2968395948410034\n",
      "Epoch: 1270/10000 Loss (MSE):  train=0.2953445315361023\n",
      "Epoch: 1280/10000 Loss (MSE):  train=0.29384845495224\n",
      "Epoch: 1290/10000 Loss (MSE):  train=0.29235154390335083\n",
      "Epoch: 1300/10000 Loss (MSE):  train=0.2908543646335602\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 1310/10000 Loss (MSE):  train=0.2893562316894531\n",
      "Epoch: 1320/10000 Loss (MSE):  train=0.2878575921058655\n",
      "Epoch: 1330/10000 Loss (MSE):  train=0.28635862469673157\n",
      "Epoch: 1340/10000 Loss (MSE):  train=0.2848592698574066\n",
      "Epoch: 1350/10000 Loss (MSE):  train=0.2833594083786011\n",
      "Epoch: 1360/10000 Loss (MSE):  train=0.28185907006263733\n",
      "Epoch: 1370/10000 Loss (MSE):  train=0.28035882115364075\n",
      "Epoch: 1380/10000 Loss (MSE):  train=0.2788577675819397\n",
      "Epoch: 1390/10000 Loss (MSE):  train=0.27735620737075806\n",
      "Epoch: 1400/10000 Loss (MSE):  train=0.27585408091545105\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 1410/10000 Loss (MSE):  train=0.27435171604156494\n",
      "Epoch: 1420/10000 Loss (MSE):  train=0.27284857630729675\n",
      "Epoch: 1430/10000 Loss (MSE):  train=0.2713450789451599\n",
      "Epoch: 1440/10000 Loss (MSE):  train=0.26984119415283203\n",
      "Epoch: 1450/10000 Loss (MSE):  train=0.268336683511734\n",
      "Epoch: 1460/10000 Loss (MSE):  train=0.26683172583580017\n",
      "Epoch: 1470/10000 Loss (MSE):  train=0.26532644033432007\n",
      "Epoch: 1480/10000 Loss (MSE):  train=0.26382115483283997\n",
      "Epoch: 1490/10000 Loss (MSE):  train=0.2623157799243927\n",
      "Epoch: 1500/10000 Loss (MSE):  train=0.2608105540275574\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 1510/10000 Loss (MSE):  train=0.25930559635162354\n",
      "Epoch: 1520/10000 Loss (MSE):  train=0.25780147314071655\n",
      "Epoch: 1530/10000 Loss (MSE):  train=0.2562985122203827\n",
      "Epoch: 1540/10000 Loss (MSE):  train=0.25479674339294434\n",
      "Epoch: 1550/10000 Loss (MSE):  train=0.25329673290252686\n",
      "Epoch: 1560/10000 Loss (MSE):  train=0.2517986297607422\n",
      "Epoch: 1570/10000 Loss (MSE):  train=0.25030359625816345\n",
      "Epoch: 1580/10000 Loss (MSE):  train=0.24881136417388916\n",
      "Epoch: 1590/10000 Loss (MSE):  train=0.24732257425785065\n",
      "Epoch: 1600/10000 Loss (MSE):  train=0.2458382248878479\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 1610/10000 Loss (MSE):  train=0.2443585991859436\n",
      "Epoch: 1620/10000 Loss (MSE):  train=0.24288392066955566\n",
      "Epoch: 1630/10000 Loss (MSE):  train=0.2414153516292572\n",
      "Epoch: 1640/10000 Loss (MSE):  train=0.23995305597782135\n",
      "Epoch: 1650/10000 Loss (MSE):  train=0.23849788308143616\n",
      "Epoch: 1660/10000 Loss (MSE):  train=0.2370501607656479\n",
      "Epoch: 1670/10000 Loss (MSE):  train=0.2356107532978058\n",
      "Epoch: 1680/10000 Loss (MSE):  train=0.23417988419532776\n",
      "Epoch: 1690/10000 Loss (MSE):  train=0.23275823891162872\n",
      "Epoch: 1700/10000 Loss (MSE):  train=0.2313464730978012\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 1710/10000 Loss (MSE):  train=0.2299450784921646\n",
      "Epoch: 1720/10000 Loss (MSE):  train=0.22855433821678162\n",
      "Epoch: 1730/10000 Loss (MSE):  train=0.22717435657978058\n",
      "Epoch: 1740/10000 Loss (MSE):  train=0.2258066087961197\n",
      "Epoch: 1750/10000 Loss (MSE):  train=0.22445029020309448\n",
      "Epoch: 1760/10000 Loss (MSE):  train=0.22310657799243927\n",
      "Epoch: 1770/10000 Loss (MSE):  train=0.22177530825138092\n",
      "Epoch: 1780/10000 Loss (MSE):  train=0.22045713663101196\n",
      "Epoch: 1790/10000 Loss (MSE):  train=0.21915195882320404\n",
      "Epoch: 1800/10000 Loss (MSE):  train=0.21786031126976013\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 1810/10000 Loss (MSE):  train=0.21658165752887726\n",
      "Epoch: 1820/10000 Loss (MSE):  train=0.2153169810771942\n",
      "Epoch: 1830/10000 Loss (MSE):  train=0.21406574547290802\n",
      "Epoch: 1840/10000 Loss (MSE):  train=0.21282818913459778\n",
      "Epoch: 1850/10000 Loss (MSE):  train=0.21160423755645752\n",
      "Epoch: 1860/10000 Loss (MSE):  train=0.21039414405822754\n",
      "Epoch: 1870/10000 Loss (MSE):  train=0.20919756591320038\n",
      "Epoch: 1880/10000 Loss (MSE):  train=0.2080146223306656\n",
      "Epoch: 1890/10000 Loss (MSE):  train=0.2068449705839157\n",
      "Epoch: 1900/10000 Loss (MSE):  train=0.20568829774856567\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 1910/10000 Loss (MSE):  train=0.2045448124408722\n",
      "Epoch: 1920/10000 Loss (MSE):  train=0.2034141719341278\n",
      "Epoch: 1930/10000 Loss (MSE):  train=0.20229598879814148\n",
      "Epoch: 1940/10000 Loss (MSE):  train=0.2011902779340744\n",
      "Epoch: 1950/10000 Loss (MSE):  train=0.20009642839431763\n",
      "Epoch: 1960/10000 Loss (MSE):  train=0.19901442527770996\n",
      "Epoch: 1970/10000 Loss (MSE):  train=0.19794395565986633\n",
      "Epoch: 1980/10000 Loss (MSE):  train=0.19688469171524048\n",
      "Epoch: 1990/10000 Loss (MSE):  train=0.1958363801240921\n",
      "Epoch: 2000/10000 Loss (MSE):  train=0.1947983205318451\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sureshjyoti/GitHub/BGNODE_scratch/.venv_jaxbrow/lib/python3.9/site-packages/shadow/plot.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(figsize=figsize, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2010/10000 Loss (MSE):  train=0.19377072155475616\n",
      "Epoch: 2020/10000 Loss (MSE):  train=0.19275297224521637\n",
      "Epoch: 2030/10000 Loss (MSE):  train=0.19174513220787048\n",
      "Epoch: 2040/10000 Loss (MSE):  train=0.1907464861869812\n",
      "Epoch: 2050/10000 Loss (MSE):  train=0.18975713849067688\n",
      "Epoch: 2060/10000 Loss (MSE):  train=0.1887764036655426\n",
      "Epoch: 2070/10000 Loss (MSE):  train=0.18780449032783508\n",
      "Epoch: 2080/10000 Loss (MSE):  train=0.18684086203575134\n",
      "Epoch: 2090/10000 Loss (MSE):  train=0.1858852356672287\n",
      "Epoch: 2100/10000 Loss (MSE):  train=0.18493735790252686\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 2110/10000 Loss (MSE):  train=0.18399715423583984\n",
      "Epoch: 2120/10000 Loss (MSE):  train=0.1830643266439438\n",
      "Epoch: 2130/10000 Loss (MSE):  train=0.18213880062103271\n",
      "Epoch: 2140/10000 Loss (MSE):  train=0.18122032284736633\n",
      "Epoch: 2150/10000 Loss (MSE):  train=0.1803087592124939\n",
      "Epoch: 2160/10000 Loss (MSE):  train=0.17940391600131989\n",
      "Epoch: 2170/10000 Loss (MSE):  train=0.17850562930107117\n",
      "Epoch: 2180/10000 Loss (MSE):  train=0.17761391401290894\n",
      "Epoch: 2190/10000 Loss (MSE):  train=0.17672841250896454\n",
      "Epoch: 2200/10000 Loss (MSE):  train=0.17584899067878723\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 2210/10000 Loss (MSE):  train=0.17497609555721283\n",
      "Epoch: 2220/10000 Loss (MSE):  train=0.17410887777805328\n",
      "Epoch: 2230/10000 Loss (MSE):  train=0.17324788868427277\n",
      "Epoch: 2240/10000 Loss (MSE):  train=0.17239275574684143\n",
      "Epoch: 2250/10000 Loss (MSE):  train=0.17154324054718018\n",
      "Epoch: 2260/10000 Loss (MSE):  train=0.17069977521896362\n",
      "Epoch: 2270/10000 Loss (MSE):  train=0.16986185312271118\n",
      "Epoch: 2280/10000 Loss (MSE):  train=0.16902939975261688\n",
      "Epoch: 2290/10000 Loss (MSE):  train=0.16820277273654938\n",
      "Epoch: 2300/10000 Loss (MSE):  train=0.1673814356327057\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 2310/10000 Loss (MSE):  train=0.16656556725502014\n",
      "Epoch: 2320/10000 Loss (MSE):  train=0.16575534641742706\n",
      "Epoch: 2330/10000 Loss (MSE):  train=0.1649501919746399\n",
      "Epoch: 2340/10000 Loss (MSE):  train=0.16415029764175415\n",
      "Epoch: 2350/10000 Loss (MSE):  train=0.16335603594779968\n",
      "Epoch: 2360/10000 Loss (MSE):  train=0.16256661713123322\n",
      "Epoch: 2370/10000 Loss (MSE):  train=0.16178223490715027\n",
      "Epoch: 2380/10000 Loss (MSE):  train=0.16100291907787323\n",
      "Epoch: 2390/10000 Loss (MSE):  train=0.16022884845733643\n",
      "Epoch: 2400/10000 Loss (MSE):  train=0.15945947170257568\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 2410/10000 Loss (MSE):  train=0.1586950421333313\n",
      "Epoch: 2420/10000 Loss (MSE):  train=0.1579352617263794\n",
      "Epoch: 2430/10000 Loss (MSE):  train=0.15718038380146027\n",
      "Epoch: 2440/10000 Loss (MSE):  train=0.15643012523651123\n",
      "Epoch: 2450/10000 Loss (MSE):  train=0.15568450093269348\n",
      "Epoch: 2460/10000 Loss (MSE):  train=0.1549435406923294\n",
      "Epoch: 2470/10000 Loss (MSE):  train=0.15420737862586975\n",
      "Epoch: 2480/10000 Loss (MSE):  train=0.15347546339035034\n",
      "Epoch: 2490/10000 Loss (MSE):  train=0.15274836122989655\n",
      "Epoch: 2500/10000 Loss (MSE):  train=0.15202555060386658\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 2510/10000 Loss (MSE):  train=0.15130740404129028\n",
      "Epoch: 2520/10000 Loss (MSE):  train=0.15059411525726318\n",
      "Epoch: 2530/10000 Loss (MSE):  train=0.1498851180076599\n",
      "Epoch: 2540/10000 Loss (MSE):  train=0.1491808295249939\n",
      "Epoch: 2550/10000 Loss (MSE):  train=0.14848148822784424\n",
      "Epoch: 2560/10000 Loss (MSE):  train=0.14778679609298706\n",
      "Epoch: 2570/10000 Loss (MSE):  train=0.14709700644016266\n",
      "Epoch: 2580/10000 Loss (MSE):  train=0.1464121788740158\n",
      "Epoch: 2590/10000 Loss (MSE):  train=0.14573237299919128\n",
      "Epoch: 2600/10000 Loss (MSE):  train=0.14505773782730103\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 2610/10000 Loss (MSE):  train=0.14438846707344055\n",
      "Epoch: 2620/10000 Loss (MSE):  train=0.1437245011329651\n",
      "Epoch: 2630/10000 Loss (MSE):  train=0.1430661529302597\n",
      "Epoch: 2640/10000 Loss (MSE):  train=0.14241349697113037\n",
      "Epoch: 2650/10000 Loss (MSE):  train=0.14176668226718903\n",
      "Epoch: 2660/10000 Loss (MSE):  train=0.14112582802772522\n",
      "Epoch: 2670/10000 Loss (MSE):  train=0.1404910832643509\n",
      "Epoch: 2680/10000 Loss (MSE):  train=0.13986234366893768\n",
      "Epoch: 2690/10000 Loss (MSE):  train=0.13923998177051544\n",
      "Epoch: 2700/10000 Loss (MSE):  train=0.13862425088882446\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 2710/10000 Loss (MSE):  train=0.13801506161689758\n",
      "Epoch: 2720/10000 Loss (MSE):  train=0.137412428855896\n",
      "Epoch: 2730/10000 Loss (MSE):  train=0.13681674003601074\n",
      "Epoch: 2740/10000 Loss (MSE):  train=0.1362278163433075\n",
      "Epoch: 2750/10000 Loss (MSE):  train=0.13564589619636536\n",
      "Epoch: 2760/10000 Loss (MSE):  train=0.13507096469402313\n",
      "Epoch: 2770/10000 Loss (MSE):  train=0.1345033049583435\n",
      "Epoch: 2780/10000 Loss (MSE):  train=0.13394272327423096\n",
      "Epoch: 2790/10000 Loss (MSE):  train=0.13338938355445862\n",
      "Epoch: 2800/10000 Loss (MSE):  train=0.13284319639205933\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 2810/10000 Loss (MSE):  train=0.1323043256998062\n",
      "Epoch: 2820/10000 Loss (MSE):  train=0.13177259266376495\n",
      "Epoch: 2830/10000 Loss (MSE):  train=0.13124828040599823\n",
      "Epoch: 2840/10000 Loss (MSE):  train=0.13073104619979858\n",
      "Epoch: 2850/10000 Loss (MSE):  train=0.13022124767303467\n",
      "Epoch: 2860/10000 Loss (MSE):  train=0.12971831858158112\n",
      "Epoch: 2870/10000 Loss (MSE):  train=0.12922267615795135\n",
      "Epoch: 2880/10000 Loss (MSE):  train=0.12873415648937225\n",
      "Epoch: 2890/10000 Loss (MSE):  train=0.1282525509595871\n",
      "Epoch: 2900/10000 Loss (MSE):  train=0.12777790427207947\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 2910/10000 Loss (MSE):  train=0.12731006741523743\n",
      "Epoch: 2920/10000 Loss (MSE):  train=0.1268487572669983\n",
      "Epoch: 2930/10000 Loss (MSE):  train=0.12639445066452026\n",
      "Epoch: 2940/10000 Loss (MSE):  train=0.12594644725322723\n",
      "Epoch: 2950/10000 Loss (MSE):  train=0.12550485134124756\n",
      "Epoch: 2960/10000 Loss (MSE):  train=0.1250695288181305\n",
      "Epoch: 2970/10000 Loss (MSE):  train=0.12464042007923126\n",
      "Epoch: 2980/10000 Loss (MSE):  train=0.12421733140945435\n",
      "Epoch: 2990/10000 Loss (MSE):  train=0.12380008399486542\n",
      "Epoch: 3000/10000 Loss (MSE):  train=0.12338864803314209\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 3010/10000 Loss (MSE):  train=0.12298275530338287\n",
      "Epoch: 3020/10000 Loss (MSE):  train=0.12258225679397583\n",
      "Epoch: 3030/10000 Loss (MSE):  train=0.12218721210956573\n",
      "Epoch: 3040/10000 Loss (MSE):  train=0.12179726362228394\n",
      "Epoch: 3050/10000 Loss (MSE):  train=0.1214122325181961\n",
      "Epoch: 3060/10000 Loss (MSE):  train=0.12103208154439926\n",
      "Epoch: 3070/10000 Loss (MSE):  train=0.12065663933753967\n",
      "Epoch: 3080/10000 Loss (MSE):  train=0.12028565257787704\n",
      "Epoch: 3090/10000 Loss (MSE):  train=0.11991903185844421\n",
      "Epoch: 3100/10000 Loss (MSE):  train=0.11955679953098297\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 3110/10000 Loss (MSE):  train=0.11919848620891571\n",
      "Epoch: 3120/10000 Loss (MSE):  train=0.11884412169456482\n",
      "Epoch: 3130/10000 Loss (MSE):  train=0.11849337816238403\n",
      "Epoch: 3140/10000 Loss (MSE):  train=0.1181463748216629\n",
      "Epoch: 3150/10000 Loss (MSE):  train=0.11780273914337158\n",
      "Epoch: 3160/10000 Loss (MSE):  train=0.11746221780776978\n",
      "Epoch: 3170/10000 Loss (MSE):  train=0.11712506413459778\n",
      "Epoch: 3180/10000 Loss (MSE):  train=0.1167907565832138\n",
      "Epoch: 3190/10000 Loss (MSE):  train=0.1164594441652298\n",
      "Epoch: 3200/10000 Loss (MSE):  train=0.11613069474697113\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 3210/10000 Loss (MSE):  train=0.11580447852611542\n",
      "Epoch: 3220/10000 Loss (MSE):  train=0.11548084020614624\n",
      "Epoch: 3230/10000 Loss (MSE):  train=0.11515912413597107\n",
      "Epoch: 3240/10000 Loss (MSE):  train=0.11483971029520035\n",
      "Epoch: 3250/10000 Loss (MSE):  train=0.11452236026525497\n",
      "Epoch: 3260/10000 Loss (MSE):  train=0.11420659720897675\n",
      "Epoch: 3270/10000 Loss (MSE):  train=0.11389272660017014\n",
      "Epoch: 3280/10000 Loss (MSE):  train=0.1135805994272232\n",
      "Epoch: 3290/10000 Loss (MSE):  train=0.11326977610588074\n",
      "Epoch: 3300/10000 Loss (MSE):  train=0.11296036839485168\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 3310/10000 Loss (MSE):  train=0.11265210062265396\n",
      "Epoch: 3320/10000 Loss (MSE):  train=0.11234481632709503\n",
      "Epoch: 3330/10000 Loss (MSE):  train=0.11203883588314056\n",
      "Epoch: 3340/10000 Loss (MSE):  train=0.11173352599143982\n",
      "Epoch: 3350/10000 Loss (MSE):  train=0.11142910271883011\n",
      "Epoch: 3360/10000 Loss (MSE):  train=0.1111253872513771\n",
      "Epoch: 3370/10000 Loss (MSE):  train=0.1108221709728241\n",
      "Epoch: 3380/10000 Loss (MSE):  train=0.11051944643259048\n",
      "Epoch: 3390/10000 Loss (MSE):  train=0.11021718382835388\n",
      "Epoch: 3400/10000 Loss (MSE):  train=0.10991518199443817\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 3410/10000 Loss (MSE):  train=0.109613336622715\n",
      "Epoch: 3420/10000 Loss (MSE):  train=0.10931164026260376\n",
      "Epoch: 3430/10000 Loss (MSE):  train=0.10901010036468506\n",
      "Epoch: 3440/10000 Loss (MSE):  train=0.10870841890573502\n",
      "Epoch: 3450/10000 Loss (MSE):  train=0.10840672254562378\n",
      "Epoch: 3460/10000 Loss (MSE):  train=0.10810483247041702\n",
      "Epoch: 3470/10000 Loss (MSE):  train=0.10780283808708191\n",
      "Epoch: 3480/10000 Loss (MSE):  train=0.10750046372413635\n",
      "Epoch: 3490/10000 Loss (MSE):  train=0.10719775408506393\n",
      "Epoch: 3500/10000 Loss (MSE):  train=0.10689482092857361\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 3510/10000 Loss (MSE):  train=0.1065913513302803\n",
      "Epoch: 3520/10000 Loss (MSE):  train=0.10628743469715118\n",
      "Epoch: 3530/10000 Loss (MSE):  train=0.10598303377628326\n",
      "Epoch: 3540/10000 Loss (MSE):  train=0.10567809641361237\n",
      "Epoch: 3550/10000 Loss (MSE):  train=0.10537263751029968\n",
      "Epoch: 3560/10000 Loss (MSE):  train=0.10506665706634521\n",
      "Epoch: 3570/10000 Loss (MSE):  train=0.10476009547710419\n",
      "Epoch: 3580/10000 Loss (MSE):  train=0.10445285588502884\n",
      "Epoch: 3590/10000 Loss (MSE):  train=0.10414505749940872\n",
      "Epoch: 3600/10000 Loss (MSE):  train=0.10383658856153488\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 3610/10000 Loss (MSE):  train=0.10352755337953568\n",
      "Epoch: 3620/10000 Loss (MSE):  train=0.10321789234876633\n",
      "Epoch: 3630/10000 Loss (MSE):  train=0.10290762782096863\n",
      "Epoch: 3640/10000 Loss (MSE):  train=0.10259681940078735\n",
      "Epoch: 3650/10000 Loss (MSE):  train=0.10228544473648071\n",
      "Epoch: 3660/10000 Loss (MSE):  train=0.10197342187166214\n",
      "Epoch: 3670/10000 Loss (MSE):  train=0.10166103392839432\n",
      "Epoch: 3680/10000 Loss (MSE):  train=0.10134803503751755\n",
      "Epoch: 3690/10000 Loss (MSE):  train=0.10103466361761093\n",
      "Epoch: 3700/10000 Loss (MSE):  train=0.1007208377122879\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 3710/10000 Loss (MSE):  train=0.10040664672851562\n",
      "Epoch: 3720/10000 Loss (MSE):  train=0.10009212046861649\n",
      "Epoch: 3730/10000 Loss (MSE):  train=0.09977712482213974\n",
      "Epoch: 3740/10000 Loss (MSE):  train=0.09946218878030777\n",
      "Epoch: 3750/10000 Loss (MSE):  train=0.09914688766002655\n",
      "Epoch: 3760/10000 Loss (MSE):  train=0.09883145242929459\n",
      "Epoch: 3770/10000 Loss (MSE):  train=0.09851594269275665\n",
      "Epoch: 3780/10000 Loss (MSE):  train=0.09820038825273514\n",
      "Epoch: 3790/10000 Loss (MSE):  train=0.0978848859667778\n",
      "Epoch: 3800/10000 Loss (MSE):  train=0.09756951779127121\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 3810/10000 Loss (MSE):  train=0.0972541943192482\n",
      "Epoch: 3820/10000 Loss (MSE):  train=0.09693922847509384\n",
      "Epoch: 3830/10000 Loss (MSE):  train=0.09662439674139023\n",
      "Epoch: 3840/10000 Loss (MSE):  train=0.09630997478961945\n",
      "Epoch: 3850/10000 Loss (MSE):  train=0.09599581360816956\n",
      "Epoch: 3860/10000 Loss (MSE):  train=0.09568220376968384\n",
      "Epoch: 3870/10000 Loss (MSE):  train=0.09536910057067871\n",
      "Epoch: 3880/10000 Loss (MSE):  train=0.09505658596754074\n",
      "Epoch: 3890/10000 Loss (MSE):  train=0.09474466741085052\n",
      "Epoch: 3900/10000 Loss (MSE):  train=0.09443342685699463\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 3910/10000 Loss (MSE):  train=0.09412303566932678\n",
      "Epoch: 3920/10000 Loss (MSE):  train=0.09381334483623505\n",
      "Epoch: 3930/10000 Loss (MSE):  train=0.09350445866584778\n",
      "Epoch: 3940/10000 Loss (MSE):  train=0.09319663047790527\n",
      "Epoch: 3950/10000 Loss (MSE):  train=0.09288974106311798\n",
      "Epoch: 3960/10000 Loss (MSE):  train=0.09258390218019485\n",
      "Epoch: 3970/10000 Loss (MSE):  train=0.09227903187274933\n",
      "Epoch: 3980/10000 Loss (MSE):  train=0.09197528660297394\n",
      "Epoch: 3990/10000 Loss (MSE):  train=0.09167276322841644\n",
      "Epoch: 4000/10000 Loss (MSE):  train=0.09137137234210968\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 4010/10000 Loss (MSE):  train=0.09107128530740738\n",
      "Epoch: 4020/10000 Loss (MSE):  train=0.09077244251966476\n",
      "Epoch: 4030/10000 Loss (MSE):  train=0.09047505259513855\n",
      "Epoch: 4040/10000 Loss (MSE):  train=0.09017892181873322\n",
      "Epoch: 4050/10000 Loss (MSE):  train=0.08988423645496368\n",
      "Epoch: 4060/10000 Loss (MSE):  train=0.08959101885557175\n",
      "Epoch: 4070/10000 Loss (MSE):  train=0.08929933607578278\n",
      "Epoch: 4080/10000 Loss (MSE):  train=0.0890091210603714\n",
      "Epoch: 4090/10000 Loss (MSE):  train=0.08872048556804657\n",
      "Epoch: 4100/10000 Loss (MSE):  train=0.08843356370925903\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 4110/10000 Loss (MSE):  train=0.08814824372529984\n",
      "Epoch: 4120/10000 Loss (MSE):  train=0.08786448836326599\n",
      "Epoch: 4130/10000 Loss (MSE):  train=0.08758257329463959\n",
      "Epoch: 4140/10000 Loss (MSE):  train=0.08730246126651764\n",
      "Epoch: 4150/10000 Loss (MSE):  train=0.0870240181684494\n",
      "Epoch: 4160/10000 Loss (MSE):  train=0.08674749732017517\n",
      "Epoch: 4170/10000 Loss (MSE):  train=0.08647294342517853\n",
      "Epoch: 4180/10000 Loss (MSE):  train=0.08620025962591171\n",
      "Epoch: 4190/10000 Loss (MSE):  train=0.08592943847179413\n",
      "Epoch: 4200/10000 Loss (MSE):  train=0.08566078543663025\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 4210/10000 Loss (MSE):  train=0.085393987596035\n",
      "Epoch: 4220/10000 Loss (MSE):  train=0.08512935042381287\n",
      "Epoch: 4230/10000 Loss (MSE):  train=0.08486688137054443\n",
      "Epoch: 4240/10000 Loss (MSE):  train=0.08460657298564911\n",
      "Epoch: 4250/10000 Loss (MSE):  train=0.08434857428073883\n",
      "Epoch: 4260/10000 Loss (MSE):  train=0.08409270644187927\n",
      "Epoch: 4270/10000 Loss (MSE):  train=0.08383922278881073\n",
      "Epoch: 4280/10000 Loss (MSE):  train=0.08358796685934067\n",
      "Epoch: 4290/10000 Loss (MSE):  train=0.0833391472697258\n",
      "Epoch: 4300/10000 Loss (MSE):  train=0.08309274911880493\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 4310/10000 Loss (MSE):  train=0.08284880965948105\n",
      "Epoch: 4320/10000 Loss (MSE):  train=0.08260729908943176\n",
      "Epoch: 4330/10000 Loss (MSE):  train=0.08236830681562424\n",
      "Epoch: 4340/10000 Loss (MSE):  train=0.08213187754154205\n",
      "Epoch: 4350/10000 Loss (MSE):  train=0.08189806342124939\n",
      "Epoch: 4360/10000 Loss (MSE):  train=0.08166682720184326\n",
      "Epoch: 4370/10000 Loss (MSE):  train=0.08143816143274307\n",
      "Epoch: 4380/10000 Loss (MSE):  train=0.08121234178543091\n",
      "Epoch: 4390/10000 Loss (MSE):  train=0.08098894357681274\n",
      "Epoch: 4400/10000 Loss (MSE):  train=0.0807684138417244\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 4410/10000 Loss (MSE):  train=0.08055061101913452\n",
      "Epoch: 4420/10000 Loss (MSE):  train=0.08033550530672073\n",
      "Epoch: 4430/10000 Loss (MSE):  train=0.08012309670448303\n",
      "Epoch: 4440/10000 Loss (MSE):  train=0.07991348206996918\n",
      "Epoch: 4450/10000 Loss (MSE):  train=0.07970672100782394\n",
      "Epoch: 4460/10000 Loss (MSE):  train=0.07950267940759659\n",
      "Epoch: 4470/10000 Loss (MSE):  train=0.07930132746696472\n",
      "Epoch: 4480/10000 Loss (MSE):  train=0.0791027694940567\n",
      "Epoch: 4490/10000 Loss (MSE):  train=0.07890696078538895\n",
      "Epoch: 4500/10000 Loss (MSE):  train=0.07871401309967041\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 4510/10000 Loss (MSE):  train=0.07852373272180557\n",
      "Epoch: 4520/10000 Loss (MSE):  train=0.07833627611398697\n",
      "Epoch: 4530/10000 Loss (MSE):  train=0.07815132290124893\n",
      "Epoch: 4540/10000 Loss (MSE):  train=0.0779692679643631\n",
      "Epoch: 4550/10000 Loss (MSE):  train=0.07778981328010559\n",
      "Epoch: 4560/10000 Loss (MSE):  train=0.07761310040950775\n",
      "Epoch: 4570/10000 Loss (MSE):  train=0.07743903994560242\n",
      "Epoch: 4580/10000 Loss (MSE):  train=0.07726752012968063\n",
      "Epoch: 4590/10000 Loss (MSE):  train=0.07709860056638718\n",
      "Epoch: 4600/10000 Loss (MSE):  train=0.07693218439817429\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 4610/10000 Loss (MSE):  train=0.07676835358142853\n",
      "Epoch: 4620/10000 Loss (MSE):  train=0.07660698890686035\n",
      "Epoch: 4630/10000 Loss (MSE):  train=0.07644806802272797\n",
      "Epoch: 4640/10000 Loss (MSE):  train=0.07629157602787018\n",
      "Epoch: 4650/10000 Loss (MSE):  train=0.07613743096590042\n",
      "Epoch: 4660/10000 Loss (MSE):  train=0.07598565518856049\n",
      "Epoch: 4670/10000 Loss (MSE):  train=0.07583615183830261\n",
      "Epoch: 4680/10000 Loss (MSE):  train=0.07568881660699844\n",
      "Epoch: 4690/10000 Loss (MSE):  train=0.07554373145103455\n",
      "Epoch: 4700/10000 Loss (MSE):  train=0.07540085911750793\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 4710/10000 Loss (MSE):  train=0.07526008784770966\n",
      "Epoch: 4720/10000 Loss (MSE):  train=0.07512131333351135\n",
      "Epoch: 4730/10000 Loss (MSE):  train=0.0749845877289772\n",
      "Epoch: 4740/10000 Loss (MSE):  train=0.07484979927539825\n",
      "Epoch: 4750/10000 Loss (MSE):  train=0.07471701502799988\n",
      "Epoch: 4760/10000 Loss (MSE):  train=0.07458611577749252\n",
      "Epoch: 4770/10000 Loss (MSE):  train=0.07445695251226425\n",
      "Epoch: 4780/10000 Loss (MSE):  train=0.07432965189218521\n",
      "Epoch: 4790/10000 Loss (MSE):  train=0.0742039605975151\n",
      "Epoch: 4800/10000 Loss (MSE):  train=0.07408010959625244\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 4810/10000 Loss (MSE):  train=0.07395784556865692\n",
      "Epoch: 4820/10000 Loss (MSE):  train=0.07383717596530914\n",
      "Epoch: 4830/10000 Loss (MSE):  train=0.07371808588504791\n",
      "Epoch: 4840/10000 Loss (MSE):  train=0.0736004188656807\n",
      "Epoch: 4850/10000 Loss (MSE):  train=0.07348428666591644\n",
      "Epoch: 4860/10000 Loss (MSE):  train=0.07336953282356262\n",
      "Epoch: 4870/10000 Loss (MSE):  train=0.07325620949268341\n",
      "Epoch: 4880/10000 Loss (MSE):  train=0.07314417511224747\n",
      "Epoch: 4890/10000 Loss (MSE):  train=0.07303345203399658\n",
      "Epoch: 4900/10000 Loss (MSE):  train=0.07292391359806061\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 4910/10000 Loss (MSE):  train=0.07281558960676193\n",
      "Epoch: 4920/10000 Loss (MSE):  train=0.07270843535661697\n",
      "Epoch: 4930/10000 Loss (MSE):  train=0.07260231673717499\n",
      "Epoch: 4940/10000 Loss (MSE):  train=0.07249736040830612\n",
      "Epoch: 4950/10000 Loss (MSE):  train=0.07239343225955963\n",
      "Epoch: 4960/10000 Loss (MSE):  train=0.07229052484035492\n",
      "Epoch: 4970/10000 Loss (MSE):  train=0.07218842208385468\n",
      "Epoch: 4980/10000 Loss (MSE):  train=0.07208728790283203\n",
      "Epoch: 4990/10000 Loss (MSE):  train=0.07198704034090042\n",
      "Epoch: 5000/10000 Loss (MSE):  train=0.07188759744167328\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 5010/10000 Loss (MSE):  train=0.07178904116153717\n",
      "Epoch: 5020/10000 Loss (MSE):  train=0.07169103622436523\n",
      "Epoch: 5030/10000 Loss (MSE):  train=0.07159386575222015\n",
      "Epoch: 5040/10000 Loss (MSE):  train=0.07149730622768402\n",
      "Epoch: 5050/10000 Loss (MSE):  train=0.071401447057724\n",
      "Epoch: 5060/10000 Loss (MSE):  train=0.07130616158246994\n",
      "Epoch: 5070/10000 Loss (MSE):  train=0.07121139019727707\n",
      "Epoch: 5080/10000 Loss (MSE):  train=0.07111713290214539\n",
      "Epoch: 5090/10000 Loss (MSE):  train=0.0710233598947525\n",
      "Epoch: 5100/10000 Loss (MSE):  train=0.07093007862567902\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 5110/10000 Loss (MSE):  train=0.07083718478679657\n",
      "Epoch: 5120/10000 Loss (MSE):  train=0.07074454426765442\n",
      "Epoch: 5130/10000 Loss (MSE):  train=0.07065232843160629\n",
      "Epoch: 5140/10000 Loss (MSE):  train=0.07056037336587906\n",
      "Epoch: 5150/10000 Loss (MSE):  train=0.07046867161989212\n",
      "Epoch: 5160/10000 Loss (MSE):  train=0.0703771561384201\n",
      "Epoch: 5170/10000 Loss (MSE):  train=0.07028589397668839\n",
      "Epoch: 5180/10000 Loss (MSE):  train=0.07019464671611786\n",
      "Epoch: 5190/10000 Loss (MSE):  train=0.07010362297296524\n",
      "Epoch: 5200/10000 Loss (MSE):  train=0.07001256942749023\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 5210/10000 Loss (MSE):  train=0.06992150098085403\n",
      "Epoch: 5220/10000 Loss (MSE):  train=0.0698305070400238\n",
      "Epoch: 5230/10000 Loss (MSE):  train=0.06973937153816223\n",
      "Epoch: 5240/10000 Loss (MSE):  train=0.06964819878339767\n",
      "Epoch: 5250/10000 Loss (MSE):  train=0.06955686211585999\n",
      "Epoch: 5260/10000 Loss (MSE):  train=0.06946536153554916\n",
      "Epoch: 5270/10000 Loss (MSE):  train=0.06937357783317566\n",
      "Epoch: 5280/10000 Loss (MSE):  train=0.06928152590990067\n",
      "Epoch: 5290/10000 Loss (MSE):  train=0.06918926537036896\n",
      "Epoch: 5300/10000 Loss (MSE):  train=0.0690966472029686\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 5310/10000 Loss (MSE):  train=0.06900355219841003\n",
      "Epoch: 5320/10000 Loss (MSE):  train=0.0689101293683052\n",
      "Epoch: 5330/10000 Loss (MSE):  train=0.06881612539291382\n",
      "Epoch: 5340/10000 Loss (MSE):  train=0.06872175633907318\n",
      "Epoch: 5350/10000 Loss (MSE):  train=0.06862674653530121\n",
      "Epoch: 5360/10000 Loss (MSE):  train=0.06853112578392029\n",
      "Epoch: 5370/10000 Loss (MSE):  train=0.06843487918376923\n",
      "Epoch: 5380/10000 Loss (MSE):  train=0.06833789497613907\n",
      "Epoch: 5390/10000 Loss (MSE):  train=0.06824028491973877\n",
      "Epoch: 5400/10000 Loss (MSE):  train=0.06814176589250565\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 5410/10000 Loss (MSE):  train=0.06804248690605164\n",
      "Epoch: 5420/10000 Loss (MSE):  train=0.06794235110282898\n",
      "Epoch: 5430/10000 Loss (MSE):  train=0.06784128397703171\n",
      "Epoch: 5440/10000 Loss (MSE):  train=0.06773914396762848\n",
      "Epoch: 5450/10000 Loss (MSE):  train=0.06763608008623123\n",
      "Epoch: 5460/10000 Loss (MSE):  train=0.06753193587064743\n",
      "Epoch: 5470/10000 Loss (MSE):  train=0.0674266666173935\n",
      "Epoch: 5480/10000 Loss (MSE):  train=0.06732029467821121\n",
      "Epoch: 5490/10000 Loss (MSE):  train=0.06721267849206924\n",
      "Epoch: 5500/10000 Loss (MSE):  train=0.06710371375083923\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 5510/10000 Loss (MSE):  train=0.06699354946613312\n",
      "Epoch: 5520/10000 Loss (MSE):  train=0.06688201427459717\n",
      "Epoch: 5530/10000 Loss (MSE):  train=0.066769078373909\n",
      "Epoch: 5540/10000 Loss (MSE):  train=0.06665463000535965\n",
      "Epoch: 5550/10000 Loss (MSE):  train=0.06653884798288345\n",
      "Epoch: 5560/10000 Loss (MSE):  train=0.0664212629199028\n",
      "Epoch: 5570/10000 Loss (MSE):  train=0.06630227714776993\n",
      "Epoch: 5580/10000 Loss (MSE):  train=0.06618163734674454\n",
      "Epoch: 5590/10000 Loss (MSE):  train=0.06605929136276245\n",
      "Epoch: 5600/10000 Loss (MSE):  train=0.06593520194292068\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 5610/10000 Loss (MSE):  train=0.06580939888954163\n",
      "Epoch: 5620/10000 Loss (MSE):  train=0.06568174809217453\n",
      "Epoch: 5630/10000 Loss (MSE):  train=0.06555229425430298\n",
      "Epoch: 5640/10000 Loss (MSE):  train=0.06542094051837921\n",
      "Epoch: 5650/10000 Loss (MSE):  train=0.0652877688407898\n",
      "Epoch: 5660/10000 Loss (MSE):  train=0.06515255570411682\n",
      "Epoch: 5670/10000 Loss (MSE):  train=0.06501544266939163\n",
      "Epoch: 5680/10000 Loss (MSE):  train=0.06487628817558289\n",
      "Epoch: 5690/10000 Loss (MSE):  train=0.06473518908023834\n",
      "Epoch: 5700/10000 Loss (MSE):  train=0.06459197402000427\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 5710/10000 Loss (MSE):  train=0.06444667279720306\n",
      "Epoch: 5720/10000 Loss (MSE):  train=0.06429940462112427\n",
      "Epoch: 5730/10000 Loss (MSE):  train=0.06415004283189774\n",
      "Epoch: 5740/10000 Loss (MSE):  train=0.06399860233068466\n",
      "Epoch: 5750/10000 Loss (MSE):  train=0.06384506076574326\n",
      "Epoch: 5760/10000 Loss (MSE):  train=0.0636894702911377\n",
      "Epoch: 5770/10000 Loss (MSE):  train=0.06353189796209335\n",
      "Epoch: 5780/10000 Loss (MSE):  train=0.06337226182222366\n",
      "Epoch: 5790/10000 Loss (MSE):  train=0.06321047991514206\n",
      "Epoch: 5800/10000 Loss (MSE):  train=0.06304681301116943\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 5810/10000 Loss (MSE):  train=0.0628812164068222\n",
      "Epoch: 5820/10000 Loss (MSE):  train=0.06271370500326157\n",
      "Epoch: 5830/10000 Loss (MSE):  train=0.0625443086028099\n",
      "Epoch: 5840/10000 Loss (MSE):  train=0.062373146414756775\n",
      "Epoch: 5850/10000 Loss (MSE):  train=0.06220017001032829\n",
      "Epoch: 5860/10000 Loss (MSE):  train=0.06202557682991028\n",
      "Epoch: 5870/10000 Loss (MSE):  train=0.06184941902756691\n",
      "Epoch: 5880/10000 Loss (MSE):  train=0.0616716705262661\n",
      "Epoch: 5890/10000 Loss (MSE):  train=0.06149262934923172\n",
      "Epoch: 5900/10000 Loss (MSE):  train=0.06131216138601303\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 5910/10000 Loss (MSE):  train=0.061130475252866745\n",
      "Epoch: 5920/10000 Loss (MSE):  train=0.0609477236866951\n",
      "Epoch: 5930/10000 Loss (MSE):  train=0.06076395511627197\n",
      "Epoch: 5940/10000 Loss (MSE):  train=0.060579340904951096\n",
      "Epoch: 5950/10000 Loss (MSE):  train=0.06039400026202202\n",
      "Epoch: 5960/10000 Loss (MSE):  train=0.0602080300450325\n",
      "Epoch: 5970/10000 Loss (MSE):  train=0.060021549463272095\n",
      "Epoch: 5980/10000 Loss (MSE):  train=0.05983477830886841\n",
      "Epoch: 5990/10000 Loss (MSE):  train=0.0596478208899498\n",
      "Epoch: 6000/10000 Loss (MSE):  train=0.05946086347103119\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 6010/10000 Loss (MSE):  train=0.05927390977740288\n",
      "Epoch: 6020/10000 Loss (MSE):  train=0.05908733606338501\n",
      "Epoch: 6030/10000 Loss (MSE):  train=0.05890114977955818\n",
      "Epoch: 6040/10000 Loss (MSE):  train=0.05871553346514702\n",
      "Epoch: 6050/10000 Loss (MSE):  train=0.05853060260415077\n",
      "Epoch: 6060/10000 Loss (MSE):  train=0.058346644043922424\n",
      "Epoch: 6070/10000 Loss (MSE):  train=0.05816375091671944\n",
      "Epoch: 6080/10000 Loss (MSE):  train=0.057982027530670166\n",
      "Epoch: 6090/10000 Loss (MSE):  train=0.05780164152383804\n",
      "Epoch: 6100/10000 Loss (MSE):  train=0.05762273073196411\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 6110/10000 Loss (MSE):  train=0.05744556337594986\n",
      "Epoch: 6120/10000 Loss (MSE):  train=0.057270120829343796\n",
      "Epoch: 6130/10000 Loss (MSE):  train=0.057096634060144424\n",
      "Epoch: 6140/10000 Loss (MSE):  train=0.056925177574157715\n",
      "Epoch: 6150/10000 Loss (MSE):  train=0.05675593763589859\n",
      "Epoch: 6160/10000 Loss (MSE):  train=0.05658893287181854\n",
      "Epoch: 6170/10000 Loss (MSE):  train=0.0564243458211422\n",
      "Epoch: 6180/10000 Loss (MSE):  train=0.056262366473674774\n",
      "Epoch: 6190/10000 Loss (MSE):  train=0.05610287934541702\n",
      "Epoch: 6200/10000 Loss (MSE):  train=0.055946119129657745\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 6210/10000 Loss (MSE):  train=0.055792078375816345\n",
      "Epoch: 6220/10000 Loss (MSE):  train=0.05564091354608536\n",
      "Epoch: 6230/10000 Loss (MSE):  train=0.05549266189336777\n",
      "Epoch: 6240/10000 Loss (MSE):  train=0.055347323417663574\n",
      "Epoch: 6250/10000 Loss (MSE):  train=0.05520503222942352\n",
      "Epoch: 6260/10000 Loss (MSE):  train=0.05506569892168045\n",
      "Epoch: 6270/10000 Loss (MSE):  train=0.05492943897843361\n",
      "Epoch: 6280/10000 Loss (MSE):  train=0.05479621887207031\n",
      "Epoch: 6290/10000 Loss (MSE):  train=0.05466608703136444\n",
      "Epoch: 6300/10000 Loss (MSE):  train=0.05453900992870331\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 6310/10000 Loss (MSE):  train=0.05441506206989288\n",
      "Epoch: 6320/10000 Loss (MSE):  train=0.05429408326745033\n",
      "Epoch: 6330/10000 Loss (MSE):  train=0.05417615920305252\n",
      "Epoch: 6340/10000 Loss (MSE):  train=0.054061245173215866\n",
      "Epoch: 6350/10000 Loss (MSE):  train=0.05394931882619858\n",
      "Epoch: 6360/10000 Loss (MSE):  train=0.05384022369980812\n",
      "Epoch: 6370/10000 Loss (MSE):  train=0.053734101355075836\n",
      "Epoch: 6380/10000 Loss (MSE):  train=0.05363075062632561\n",
      "Epoch: 6390/10000 Loss (MSE):  train=0.05353018641471863\n",
      "Epoch: 6400/10000 Loss (MSE):  train=0.05343226343393326\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 6410/10000 Loss (MSE):  train=0.053337082266807556\n",
      "Epoch: 6420/10000 Loss (MSE):  train=0.05324440449476242\n",
      "Epoch: 6430/10000 Loss (MSE):  train=0.05315423756837845\n",
      "Epoch: 6440/10000 Loss (MSE):  train=0.05306647717952728\n",
      "Epoch: 6450/10000 Loss (MSE):  train=0.05298114940524101\n",
      "Epoch: 6460/10000 Loss (MSE):  train=0.052898041903972626\n",
      "Epoch: 6470/10000 Loss (MSE):  train=0.05281716585159302\n",
      "Epoch: 6480/10000 Loss (MSE):  train=0.052738457918167114\n",
      "Epoch: 6490/10000 Loss (MSE):  train=0.05266176164150238\n",
      "Epoch: 6500/10000 Loss (MSE):  train=0.05258706212043762\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 6510/10000 Loss (MSE):  train=0.05251430347561836\n",
      "Epoch: 6520/10000 Loss (MSE):  train=0.05244332179427147\n",
      "Epoch: 6530/10000 Loss (MSE):  train=0.052374161779880524\n",
      "Epoch: 6540/10000 Loss (MSE):  train=0.05230666697025299\n",
      "Epoch: 6550/10000 Loss (MSE):  train=0.05224074050784111\n",
      "Epoch: 6560/10000 Loss (MSE):  train=0.05217651277780533\n",
      "Epoch: 6570/10000 Loss (MSE):  train=0.0521136112511158\n",
      "Epoch: 6580/10000 Loss (MSE):  train=0.05205219238996506\n",
      "Epoch: 6590/10000 Loss (MSE):  train=0.05199221521615982\n",
      "Epoch: 6600/10000 Loss (MSE):  train=0.051933448761701584\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 6610/10000 Loss (MSE):  train=0.05187594145536423\n",
      "Epoch: 6620/10000 Loss (MSE):  train=0.05181954801082611\n",
      "Epoch: 6630/10000 Loss (MSE):  train=0.051764361560344696\n",
      "Epoch: 6640/10000 Loss (MSE):  train=0.05171030014753342\n",
      "Epoch: 6650/10000 Loss (MSE):  train=0.05165717378258705\n",
      "Epoch: 6660/10000 Loss (MSE):  train=0.051605064421892166\n",
      "Epoch: 6670/10000 Loss (MSE):  train=0.05155389755964279\n",
      "Epoch: 6680/10000 Loss (MSE):  train=0.05150362104177475\n",
      "Epoch: 6690/10000 Loss (MSE):  train=0.05145417898893356\n",
      "Epoch: 6700/10000 Loss (MSE):  train=0.05140557140111923\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 6710/10000 Loss (MSE):  train=0.05135773867368698\n",
      "Epoch: 6720/10000 Loss (MSE):  train=0.051310647279024124\n",
      "Epoch: 6730/10000 Loss (MSE):  train=0.05126423388719559\n",
      "Epoch: 6740/10000 Loss (MSE):  train=0.05121856927871704\n",
      "Epoch: 6750/10000 Loss (MSE):  train=0.05117350071668625\n",
      "Epoch: 6760/10000 Loss (MSE):  train=0.05112909525632858\n",
      "Epoch: 6770/10000 Loss (MSE):  train=0.05108528956770897\n",
      "Epoch: 6780/10000 Loss (MSE):  train=0.051041945815086365\n",
      "Epoch: 6790/10000 Loss (MSE):  train=0.05099927634000778\n",
      "Epoch: 6800/10000 Loss (MSE):  train=0.05095706135034561\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 6810/10000 Loss (MSE):  train=0.05091541260480881\n",
      "Epoch: 6820/10000 Loss (MSE):  train=0.05087418109178543\n",
      "Epoch: 6830/10000 Loss (MSE):  train=0.050833433866500854\n",
      "Epoch: 6840/10000 Loss (MSE):  train=0.050793133676052094\n",
      "Epoch: 6850/10000 Loss (MSE):  train=0.05075332522392273\n",
      "Epoch: 6860/10000 Loss (MSE):  train=0.05071387067437172\n",
      "Epoch: 6870/10000 Loss (MSE):  train=0.05067485570907593\n",
      "Epoch: 6880/10000 Loss (MSE):  train=0.050636231899261475\n",
      "Epoch: 6890/10000 Loss (MSE):  train=0.05059794336557388\n",
      "Epoch: 6900/10000 Loss (MSE):  train=0.05056010186672211\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 6910/10000 Loss (MSE):  train=0.05052252113819122\n",
      "Epoch: 6920/10000 Loss (MSE):  train=0.05048533156514168\n",
      "Epoch: 6930/10000 Loss (MSE):  train=0.05044850707054138\n",
      "Epoch: 6940/10000 Loss (MSE):  train=0.050411976873874664\n",
      "Epoch: 6950/10000 Loss (MSE):  train=0.050375718623399734\n",
      "Epoch: 6960/10000 Loss (MSE):  train=0.05033983662724495\n",
      "Epoch: 6970/10000 Loss (MSE):  train=0.05030421167612076\n",
      "Epoch: 6980/10000 Loss (MSE):  train=0.05026889592409134\n",
      "Epoch: 6990/10000 Loss (MSE):  train=0.05023385211825371\n",
      "Epoch: 7000/10000 Loss (MSE):  train=0.05019909143447876\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 7010/10000 Loss (MSE):  train=0.05016461759805679\n",
      "Epoch: 7020/10000 Loss (MSE):  train=0.050130344927310944\n",
      "Epoch: 7030/10000 Loss (MSE):  train=0.05009640380740166\n",
      "Epoch: 7040/10000 Loss (MSE):  train=0.050062719732522964\n",
      "Epoch: 7050/10000 Loss (MSE):  train=0.050029218196868896\n",
      "Epoch: 7060/10000 Loss (MSE):  train=0.04999598488211632\n",
      "Epoch: 7070/10000 Loss (MSE):  train=0.04996304214000702\n",
      "Epoch: 7080/10000 Loss (MSE):  train=0.049930255860090256\n",
      "Epoch: 7090/10000 Loss (MSE):  train=0.04989771917462349\n",
      "Epoch: 7100/10000 Loss (MSE):  train=0.04986542463302612\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 7110/10000 Loss (MSE):  train=0.04983338713645935\n",
      "Epoch: 7120/10000 Loss (MSE):  train=0.04980152100324631\n",
      "Epoch: 7130/10000 Loss (MSE):  train=0.049769915640354156\n",
      "Epoch: 7140/10000 Loss (MSE):  train=0.049738410860300064\n",
      "Epoch: 7150/10000 Loss (MSE):  train=0.04970723018050194\n",
      "Epoch: 7160/10000 Loss (MSE):  train=0.049676209688186646\n",
      "Epoch: 7170/10000 Loss (MSE):  train=0.049645375460386276\n",
      "Epoch: 7180/10000 Loss (MSE):  train=0.04961470514535904\n",
      "Epoch: 7190/10000 Loss (MSE):  train=0.049584273248910904\n",
      "Epoch: 7200/10000 Loss (MSE):  train=0.04955407977104187\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 7210/10000 Loss (MSE):  train=0.04952404648065567\n",
      "Epoch: 7220/10000 Loss (MSE):  train=0.04949415847659111\n",
      "Epoch: 7230/10000 Loss (MSE):  train=0.04946446791291237\n",
      "Epoch: 7240/10000 Loss (MSE):  train=0.04943501204252243\n",
      "Epoch: 7250/10000 Loss (MSE):  train=0.04940571263432503\n",
      "Epoch: 7260/10000 Loss (MSE):  train=0.04937654733657837\n",
      "Epoch: 7270/10000 Loss (MSE):  train=0.049347616732120514\n",
      "Epoch: 7280/10000 Loss (MSE):  train=0.049318816512823105\n",
      "Epoch: 7290/10000 Loss (MSE):  train=0.04929027706384659\n",
      "Epoch: 7300/10000 Loss (MSE):  train=0.049261800944805145\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 7310/10000 Loss (MSE):  train=0.0492335669696331\n",
      "Epoch: 7320/10000 Loss (MSE):  train=0.04920542985200882\n",
      "Epoch: 7330/10000 Loss (MSE):  train=0.04917747527360916\n",
      "Epoch: 7340/10000 Loss (MSE):  train=0.0491497702896595\n",
      "Epoch: 7350/10000 Loss (MSE):  train=0.04912208765745163\n",
      "Epoch: 7360/10000 Loss (MSE):  train=0.04909466579556465\n",
      "Epoch: 7370/10000 Loss (MSE):  train=0.04906739667057991\n",
      "Epoch: 7380/10000 Loss (MSE):  train=0.04904026538133621\n",
      "Epoch: 7390/10000 Loss (MSE):  train=0.049013275653123856\n",
      "Epoch: 7400/10000 Loss (MSE):  train=0.048986442387104034\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 7410/10000 Loss (MSE):  train=0.048959776759147644\n",
      "Epoch: 7420/10000 Loss (MSE):  train=0.048933226615190506\n",
      "Epoch: 7430/10000 Loss (MSE):  train=0.04890685901045799\n",
      "Epoch: 7440/10000 Loss (MSE):  train=0.04888066276907921\n",
      "Epoch: 7450/10000 Loss (MSE):  train=0.048854585736989975\n",
      "Epoch: 7460/10000 Loss (MSE):  train=0.04882862791419029\n",
      "Epoch: 7470/10000 Loss (MSE):  train=0.048802848905324936\n",
      "Epoch: 7480/10000 Loss (MSE):  train=0.04877723008394241\n",
      "Epoch: 7490/10000 Loss (MSE):  train=0.04875173419713974\n",
      "Epoch: 7500/10000 Loss (MSE):  train=0.0487264022231102\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 7510/10000 Loss (MSE):  train=0.04870114102959633\n",
      "Epoch: 7520/10000 Loss (MSE):  train=0.04867606982588768\n",
      "Epoch: 7530/10000 Loss (MSE):  train=0.04865115508437157\n",
      "Epoch: 7540/10000 Loss (MSE):  train=0.048626333475112915\n",
      "Epoch: 7550/10000 Loss (MSE):  train=0.0486016720533371\n",
      "Epoch: 7560/10000 Loss (MSE):  train=0.04857717826962471\n",
      "Epoch: 7570/10000 Loss (MSE):  train=0.04855276644229889\n",
      "Epoch: 7580/10000 Loss (MSE):  train=0.04852847382426262\n",
      "Epoch: 7590/10000 Loss (MSE):  train=0.04850434511899948\n",
      "Epoch: 7600/10000 Loss (MSE):  train=0.04848039150238037\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 7610/10000 Loss (MSE):  train=0.04845648258924484\n",
      "Epoch: 7620/10000 Loss (MSE):  train=0.048432767391204834\n",
      "Epoch: 7630/10000 Loss (MSE):  train=0.04840908944606781\n",
      "Epoch: 7640/10000 Loss (MSE):  train=0.048385631293058395\n",
      "Epoch: 7650/10000 Loss (MSE):  train=0.048362262547016144\n",
      "Epoch: 7660/10000 Loss (MSE):  train=0.04833906143903732\n",
      "Epoch: 7670/10000 Loss (MSE):  train=0.04831596091389656\n",
      "Epoch: 7680/10000 Loss (MSE):  train=0.048292964696884155\n",
      "Epoch: 7690/10000 Loss (MSE):  train=0.04827010631561279\n",
      "Epoch: 7700/10000 Loss (MSE):  train=0.0482473261654377\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 7710/10000 Loss (MSE):  train=0.04822472482919693\n",
      "Epoch: 7720/10000 Loss (MSE):  train=0.04820221662521362\n",
      "Epoch: 7730/10000 Loss (MSE):  train=0.048179879784584045\n",
      "Epoch: 7740/10000 Loss (MSE):  train=0.048157595098018646\n",
      "Epoch: 7750/10000 Loss (MSE):  train=0.048135481774806976\n",
      "Epoch: 7760/10000 Loss (MSE):  train=0.04811340570449829\n",
      "Epoch: 7770/10000 Loss (MSE):  train=0.04809149354696274\n",
      "Epoch: 7780/10000 Loss (MSE):  train=0.04806968569755554\n",
      "Epoch: 7790/10000 Loss (MSE):  train=0.04804803431034088\n",
      "Epoch: 7800/10000 Loss (MSE):  train=0.048026520758867264\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 7810/10000 Loss (MSE):  train=0.04800502955913544\n",
      "Epoch: 7820/10000 Loss (MSE):  train=0.047983698546886444\n",
      "Epoch: 7830/10000 Loss (MSE):  train=0.0479624941945076\n",
      "Epoch: 7840/10000 Loss (MSE):  train=0.047941360622644424\n",
      "Epoch: 7850/10000 Loss (MSE):  train=0.04792037978768349\n",
      "Epoch: 7860/10000 Loss (MSE):  train=0.0478995144367218\n",
      "Epoch: 7870/10000 Loss (MSE):  train=0.0478786900639534\n",
      "Epoch: 7880/10000 Loss (MSE):  train=0.04785802587866783\n",
      "Epoch: 7890/10000 Loss (MSE):  train=0.0478375181555748\n",
      "Epoch: 7900/10000 Loss (MSE):  train=0.04781704768538475\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 7910/10000 Loss (MSE):  train=0.04779670014977455\n",
      "Epoch: 7920/10000 Loss (MSE):  train=0.04777643829584122\n",
      "Epoch: 7930/10000 Loss (MSE):  train=0.04775632172822952\n",
      "Epoch: 7940/10000 Loss (MSE):  train=0.04773629829287529\n",
      "Epoch: 7950/10000 Loss (MSE):  train=0.047716353088617325\n",
      "Epoch: 7960/10000 Loss (MSE):  train=0.04769658297300339\n",
      "Epoch: 7970/10000 Loss (MSE):  train=0.047676801681518555\n",
      "Epoch: 7980/10000 Loss (MSE):  train=0.04765721783041954\n",
      "Epoch: 7990/10000 Loss (MSE):  train=0.0476377010345459\n",
      "Epoch: 8000/10000 Loss (MSE):  train=0.047618262469768524\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 8010/10000 Loss (MSE):  train=0.04759897291660309\n",
      "Epoch: 8020/10000 Loss (MSE):  train=0.0475798025727272\n",
      "Epoch: 8030/10000 Loss (MSE):  train=0.04756070300936699\n",
      "Epoch: 8040/10000 Loss (MSE):  train=0.04754166677594185\n",
      "Epoch: 8050/10000 Loss (MSE):  train=0.047522734850645065\n",
      "Epoch: 8060/10000 Loss (MSE):  train=0.04750392213463783\n",
      "Epoch: 8070/10000 Loss (MSE):  train=0.04748521000146866\n",
      "Epoch: 8080/10000 Loss (MSE):  train=0.04746658727526665\n",
      "Epoch: 8090/10000 Loss (MSE):  train=0.04744803160429001\n",
      "Epoch: 8100/10000 Loss (MSE):  train=0.047429606318473816\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 8110/10000 Loss (MSE):  train=0.04741129279136658\n",
      "Epoch: 8120/10000 Loss (MSE):  train=0.0473930761218071\n",
      "Epoch: 8130/10000 Loss (MSE):  train=0.047374922782182693\n",
      "Epoch: 8140/10000 Loss (MSE):  train=0.04735687002539635\n",
      "Epoch: 8150/10000 Loss (MSE):  train=0.04733889549970627\n",
      "Epoch: 8160/10000 Loss (MSE):  train=0.04732108861207962\n",
      "Epoch: 8170/10000 Loss (MSE):  train=0.04730325937271118\n",
      "Epoch: 8180/10000 Loss (MSE):  train=0.047285567969083786\n",
      "Epoch: 8190/10000 Loss (MSE):  train=0.04726800695061684\n",
      "Epoch: 8200/10000 Loss (MSE):  train=0.04725050926208496\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 8210/10000 Loss (MSE):  train=0.047233134508132935\n",
      "Epoch: 8220/10000 Loss (MSE):  train=0.04721580073237419\n",
      "Epoch: 8230/10000 Loss (MSE):  train=0.047198597341775894\n",
      "Epoch: 8240/10000 Loss (MSE):  train=0.0471813827753067\n",
      "Epoch: 8250/10000 Loss (MSE):  train=0.047164347022771835\n",
      "Epoch: 8260/10000 Loss (MSE):  train=0.04714735224843025\n",
      "Epoch: 8270/10000 Loss (MSE):  train=0.04713049903512001\n",
      "Epoch: 8280/10000 Loss (MSE):  train=0.047113679349422455\n",
      "Epoch: 8290/10000 Loss (MSE):  train=0.04709703475236893\n",
      "Epoch: 8300/10000 Loss (MSE):  train=0.04708033427596092\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 8310/10000 Loss (MSE):  train=0.047063834965229034\n",
      "Epoch: 8320/10000 Loss (MSE):  train=0.04704734683036804\n",
      "Epoch: 8330/10000 Loss (MSE):  train=0.0470309853553772\n",
      "Epoch: 8340/10000 Loss (MSE):  train=0.04701467603445053\n",
      "Epoch: 8350/10000 Loss (MSE):  train=0.04699844494462013\n",
      "Epoch: 8360/10000 Loss (MSE):  train=0.04698241129517555\n",
      "Epoch: 8370/10000 Loss (MSE):  train=0.04696636646986008\n",
      "Epoch: 8380/10000 Loss (MSE):  train=0.046950340270996094\n",
      "Epoch: 8390/10000 Loss (MSE):  train=0.04693445563316345\n",
      "Epoch: 8400/10000 Loss (MSE):  train=0.04691866785287857\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 8410/10000 Loss (MSE):  train=0.04690297320485115\n",
      "Epoch: 8420/10000 Loss (MSE):  train=0.04688732326030731\n",
      "Epoch: 8430/10000 Loss (MSE):  train=0.04687173292040825\n",
      "Epoch: 8440/10000 Loss (MSE):  train=0.04685631021857262\n",
      "Epoch: 8450/10000 Loss (MSE):  train=0.04684086889028549\n",
      "Epoch: 8460/10000 Loss (MSE):  train=0.04682552441954613\n",
      "Epoch: 8470/10000 Loss (MSE):  train=0.04681027680635452\n",
      "Epoch: 8480/10000 Loss (MSE):  train=0.04679512605071068\n",
      "Epoch: 8490/10000 Loss (MSE):  train=0.04678001254796982\n",
      "Epoch: 8500/10000 Loss (MSE):  train=0.046765007078647614\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 8510/10000 Loss (MSE):  train=0.0467500314116478\n",
      "Epoch: 8520/10000 Loss (MSE):  train=0.04673515260219574\n",
      "Epoch: 8530/10000 Loss (MSE):  train=0.04672037437558174\n",
      "Epoch: 8540/10000 Loss (MSE):  train=0.04670565947890282\n",
      "Epoch: 8550/10000 Loss (MSE):  train=0.046690985560417175\n",
      "Epoch: 8560/10000 Loss (MSE):  train=0.04667642340064049\n",
      "Epoch: 8570/10000 Loss (MSE):  train=0.04666191712021828\n",
      "Epoch: 8580/10000 Loss (MSE):  train=0.04664749652147293\n",
      "Epoch: 8590/10000 Loss (MSE):  train=0.046633101999759674\n",
      "Epoch: 8600/10000 Loss (MSE):  train=0.04661877453327179\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 8610/10000 Loss (MSE):  train=0.046604596078395844\n",
      "Epoch: 8620/10000 Loss (MSE):  train=0.0465904101729393\n",
      "Epoch: 8630/10000 Loss (MSE):  train=0.046576328575611115\n",
      "Epoch: 8640/10000 Loss (MSE):  train=0.04656233265995979\n",
      "Epoch: 8650/10000 Loss (MSE):  train=0.04654838144779205\n",
      "Epoch: 8660/10000 Loss (MSE):  train=0.04653448984026909\n",
      "Epoch: 8670/10000 Loss (MSE):  train=0.0465206541121006\n",
      "Epoch: 8680/10000 Loss (MSE):  train=0.04650692269206047\n",
      "Epoch: 8690/10000 Loss (MSE):  train=0.046493250876665115\n",
      "Epoch: 8700/10000 Loss (MSE):  train=0.04647964984178543\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 8710/10000 Loss (MSE):  train=0.04646613448858261\n",
      "Epoch: 8720/10000 Loss (MSE):  train=0.04645266383886337\n",
      "Epoch: 8730/10000 Loss (MSE):  train=0.046439267694950104\n",
      "Epoch: 8740/10000 Loss (MSE):  train=0.04642588272690773\n",
      "Epoch: 8750/10000 Loss (MSE):  train=0.046412594616413116\n",
      "Epoch: 8760/10000 Loss (MSE):  train=0.04639936238527298\n",
      "Epoch: 8770/10000 Loss (MSE):  train=0.04638621583580971\n",
      "Epoch: 8780/10000 Loss (MSE):  train=0.04637310281395912\n",
      "Epoch: 8790/10000 Loss (MSE):  train=0.046360116451978683\n",
      "Epoch: 8800/10000 Loss (MSE):  train=0.046347130089998245\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 8810/10000 Loss (MSE):  train=0.046334195882081985\n",
      "Epoch: 8820/10000 Loss (MSE):  train=0.04632137715816498\n",
      "Epoch: 8830/10000 Loss (MSE):  train=0.04630858823657036\n",
      "Epoch: 8840/10000 Loss (MSE):  train=0.0462958887219429\n",
      "Epoch: 8850/10000 Loss (MSE):  train=0.04628320038318634\n",
      "Epoch: 8860/10000 Loss (MSE):  train=0.04627057909965515\n",
      "Epoch: 8870/10000 Loss (MSE):  train=0.046258024871349335\n",
      "Epoch: 8880/10000 Loss (MSE):  train=0.04624556377530098\n",
      "Epoch: 8890/10000 Loss (MSE):  train=0.04623313248157501\n",
      "Epoch: 8900/10000 Loss (MSE):  train=0.046220745891332626\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 8910/10000 Loss (MSE):  train=0.04620841518044472\n",
      "Epoch: 8920/10000 Loss (MSE):  train=0.04619617015123367\n",
      "Epoch: 8930/10000 Loss (MSE):  train=0.046183958649635315\n",
      "Epoch: 8940/10000 Loss (MSE):  train=0.046171821653842926\n",
      "Epoch: 8950/10000 Loss (MSE):  train=0.046159714460372925\n",
      "Epoch: 8960/10000 Loss (MSE):  train=0.04614773392677307\n",
      "Epoch: 8970/10000 Loss (MSE):  train=0.046135708689689636\n",
      "Epoch: 8980/10000 Loss (MSE):  train=0.04612381383776665\n",
      "Epoch: 8990/10000 Loss (MSE):  train=0.04611193761229515\n",
      "Epoch: 9000/10000 Loss (MSE):  train=0.04610016196966171\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 9010/10000 Loss (MSE):  train=0.04608835652470589\n",
      "Epoch: 9020/10000 Loss (MSE):  train=0.046076659113168716\n",
      "Epoch: 9030/10000 Loss (MSE):  train=0.04606500640511513\n",
      "Epoch: 9040/10000 Loss (MSE):  train=0.04605340585112572\n",
      "Epoch: 9050/10000 Loss (MSE):  train=0.04604185000061989\n",
      "Epoch: 9060/10000 Loss (MSE):  train=0.04603036493062973\n",
      "Epoch: 9070/10000 Loss (MSE):  train=0.04601888358592987\n",
      "Epoch: 9080/10000 Loss (MSE):  train=0.046007513999938965\n",
      "Epoch: 9090/10000 Loss (MSE):  train=0.04599614441394806\n",
      "Epoch: 9100/10000 Loss (MSE):  train=0.045984845608472824\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 9110/10000 Loss (MSE):  train=0.04597359895706177\n",
      "Epoch: 9120/10000 Loss (MSE):  train=0.04596242308616638\n",
      "Epoch: 9130/10000 Loss (MSE):  train=0.04595125466585159\n",
      "Epoch: 9140/10000 Loss (MSE):  train=0.04594013839960098\n",
      "Epoch: 9150/10000 Loss (MSE):  train=0.04592911899089813\n",
      "Epoch: 9160/10000 Loss (MSE):  train=0.045918114483356476\n",
      "Epoch: 9170/10000 Loss (MSE):  train=0.045907195657491684\n",
      "Epoch: 9180/10000 Loss (MSE):  train=0.04589627683162689\n",
      "Epoch: 9190/10000 Loss (MSE):  train=0.045885391533374786\n",
      "Epoch: 9200/10000 Loss (MSE):  train=0.04587460681796074\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 9210/10000 Loss (MSE):  train=0.045863837003707886\n",
      "Epoch: 9220/10000 Loss (MSE):  train=0.04585311561822891\n",
      "Epoch: 9230/10000 Loss (MSE):  train=0.04584244266152382\n",
      "Epoch: 9240/10000 Loss (MSE):  train=0.04583179950714111\n",
      "Epoch: 9250/10000 Loss (MSE):  train=0.0458211824297905\n",
      "Epoch: 9260/10000 Loss (MSE):  train=0.045810699462890625\n",
      "Epoch: 9270/10000 Loss (MSE):  train=0.04580017924308777\n",
      "Epoch: 9280/10000 Loss (MSE):  train=0.04578973352909088\n",
      "Epoch: 9290/10000 Loss (MSE):  train=0.0457792766392231\n",
      "Epoch: 9300/10000 Loss (MSE):  train=0.04576891288161278\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 9310/10000 Loss (MSE):  train=0.04575861990451813\n",
      "Epoch: 9320/10000 Loss (MSE):  train=0.04574834182858467\n",
      "Epoch: 9330/10000 Loss (MSE):  train=0.04573807120323181\n",
      "Epoch: 9340/10000 Loss (MSE):  train=0.04572790116071701\n",
      "Epoch: 9350/10000 Loss (MSE):  train=0.04571778327226639\n",
      "Epoch: 9360/10000 Loss (MSE):  train=0.045707620680332184\n",
      "Epoch: 9370/10000 Loss (MSE):  train=0.04569758474826813\n",
      "Epoch: 9380/10000 Loss (MSE):  train=0.04568755626678467\n",
      "Epoch: 9390/10000 Loss (MSE):  train=0.0456775426864624\n",
      "Epoch: 9400/10000 Loss (MSE):  train=0.045667629688978195\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 9410/10000 Loss (MSE):  train=0.0456576943397522\n",
      "Epoch: 9420/10000 Loss (MSE):  train=0.04564782977104187\n",
      "Epoch: 9430/10000 Loss (MSE):  train=0.045638032257556915\n",
      "Epoch: 9440/10000 Loss (MSE):  train=0.045628227293491364\n",
      "Epoch: 9450/10000 Loss (MSE):  train=0.04561849683523178\n",
      "Epoch: 9460/10000 Loss (MSE):  train=0.04560878872871399\n",
      "Epoch: 9470/10000 Loss (MSE):  train=0.04559912160038948\n",
      "Epoch: 9480/10000 Loss (MSE):  train=0.04558945447206497\n",
      "Epoch: 9490/10000 Loss (MSE):  train=0.045579876750707626\n",
      "Epoch: 9500/10000 Loss (MSE):  train=0.04557030275464058\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/loss_array.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/training_loss.png ===\n",
      "Epoch: 9510/10000 Loss (MSE):  train=0.0455607995390892\n",
      "Epoch: 9520/10000 Loss (MSE):  train=0.04555131494998932\n",
      "Epoch: 9530/10000 Loss (MSE):  train=0.045541852712631226\n",
      "Epoch: 9540/10000 Loss (MSE):  train=0.04553244635462761\n",
      "Epoch: 9550/10000 Loss (MSE):  train=0.04552307724952698\n",
      "Epoch: 9560/10000 Loss (MSE):  train=0.045513734221458435\n",
      "Epoch: 9570/10000 Loss (MSE):  train=0.04550442472100258\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(bRs_in, bRs_out):\n\u001b[1;32m     65\u001b[0m     optimizer_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 66\u001b[0m     opt_state, params, l_ \u001b[39m=\u001b[39m step(\n\u001b[1;32m     67\u001b[0m         optimizer_step, (opt_state, params, \u001b[39m0\u001b[39;49m), \u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m     68\u001b[0m     l \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m l_\n\u001b[1;32m     69\u001b[0m     count\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/GitHub/BGNODE_scratch/.venv_jaxbrow/lib/python3.9/site-packages/jax/example_libraries/optimizers.py:120\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(data, xs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m# The implementation here basically works by flattening pytrees. There are two\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# levels of pytrees to think about: the pytree of params, which we can think of\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# as defining an \"outer pytree\", and a pytree produced by applying init_fun to\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# each leaf of the params pytree, which we can think of as the \"inner pytrees\".\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m# Since pytrees can be flattened, that structure is isomorphic to a list of\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m# lists (with no further nesting).\u001b[39;00m\n\u001b[1;32m    115\u001b[0m OptimizerState \u001b[39m=\u001b[39m namedtuple(\u001b[39m\"\u001b[39m\u001b[39mOptimizerState\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m                             [\u001b[39m\"\u001b[39m\u001b[39mpacked_state\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtree_def\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msubtree_defs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    117\u001b[0m register_pytree_node(\n\u001b[1;32m    118\u001b[0m     OptimizerState,\n\u001b[1;32m    119\u001b[0m     \u001b[39mlambda\u001b[39;00m xs: ((xs\u001b[39m.\u001b[39mpacked_state,), (xs\u001b[39m.\u001b[39mtree_def, xs\u001b[39m.\u001b[39msubtree_defs)),\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mlambda\u001b[39;00m data, xs: OptimizerState(xs[\u001b[39m0\u001b[39m], data[\u001b[39m0\u001b[39m], data[\u001b[39m1\u001b[39m]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m Array \u001b[39m=\u001b[39m Any\n\u001b[1;32m    124\u001b[0m Params \u001b[39m=\u001b[39m Any  \u001b[39m# Parameters are arbitrary nests of `jnp.ndarrays`.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def loss_fn(params, Rs, Rs_1_ac):\n",
    "    Rs_1_pred = v_v_next_step_fn_model(Rs, params)\n",
    "    return MSE(Rs_1_pred, Rs_1_ac)\n",
    "\n",
    "def gloss(*args):\n",
    "    return value_and_grad(loss_fn)(*args)\n",
    "\n",
    "def update(i, opt_state, params, loss__, *data):\n",
    "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "    value, grads_ = gloss(params, *data)\n",
    "    opt_state = opt_update(i, grads_, opt_state)\n",
    "    return opt_state, get_params(opt_state), value\n",
    "\n",
    "@jit\n",
    "def step(i, ps, *args):\n",
    "    return update(i, *ps, *args)\n",
    "\n",
    "opt_init, opt_update_, get_params = optimizers.adam(lr)\n",
    "\n",
    "@jit\n",
    "def opt_update(i, grads_, opt_state):\n",
    "    grads_ = jax.tree_map(jnp.nan_to_num, grads_)\n",
    "    grads_ = jax.tree_map(partial(jnp.clip, a_min=-1000.0, a_max=1000.0), grads_)\n",
    "    return opt_update_(i, grads_, opt_state)\n",
    "\n",
    "def batching(*args, size=None):\n",
    "    L = len(args[0])\n",
    "    if size != None:\n",
    "        nbatches1 = int((L - 0.5) // size) + 1\n",
    "        nbatches2 = max(1, nbatches1 - 1)\n",
    "        size1 = int(L/nbatches1)\n",
    "        size2 = int(L/nbatches2)\n",
    "        if size1*nbatches1 > size2*nbatches2:\n",
    "            size = size1\n",
    "            nbatches = nbatches1\n",
    "        else:\n",
    "            size = size2\n",
    "            nbatches = nbatches2\n",
    "    else:\n",
    "        nbatches = 1\n",
    "        size = L\n",
    "    \n",
    "    newargs = []\n",
    "    for arg in args:\n",
    "        newargs += [jnp.array([arg[i*size:(i+1)*size]\n",
    "                                for i in range(nbatches)])]\n",
    "    return newargs\n",
    "\n",
    "bRs_in, bRs_out = batching(Rs_in, Rs_out, size=min(len(Rs_in), batch_size))\n",
    "\n",
    "print(f\"training ...\")\n",
    "\n",
    "opt_state = opt_init(params)\n",
    "epoch = 0\n",
    "optimizer_step = -1\n",
    "larray = []\n",
    "ltarray = []\n",
    "last_loss = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    l = 0.0\n",
    "    count = 0\n",
    "    for data in zip(bRs_in, bRs_out):\n",
    "        optimizer_step += 1\n",
    "        opt_state, params, l_ = step(\n",
    "            optimizer_step, (opt_state, params, 0), *data)\n",
    "        l += l_\n",
    "        count+=1\n",
    "    # print(\"epoch,countttttt: \", epoch,count)\n",
    "    # opt_state, params, l_ = step(optimizer_step, (opt_state, params, 0), Rs, Vs, Fs)\n",
    "    l = l/count\n",
    "    larray += [l]\n",
    "    # ltarray += [loss_fn(params, bRs_in, bVs_in, bRs_out)]\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs} Loss (MSE):  train={larray[-1]}\")#, test={ltarray[-1]}\")\n",
    "    if epoch % 100 == 0:\n",
    "        metadata = {\n",
    "            \"savedat\": epoch,\n",
    "            # \"mpass\": mpass,\n",
    "            }\n",
    "        savefile(f\"fgnode_trained_model.dil\",\n",
    "                    params, metadata=metadata)\n",
    "        # savefile(f\"loss_array.dil\", (larray, ltarray), metadata=metadata)\n",
    "        savefile(f\"loss_array.dil\", larray, metadata=metadata)\n",
    "        if last_loss > larray[-1]:\n",
    "            last_loss = larray[-1]\n",
    "            savefile(f\"fgnode_trained_model_low.dil\",\n",
    "                        params, metadata=metadata)\n",
    "        fig, axs = panel(1, 1)\n",
    "        # plt.semilogy(larray, label=\"Training\")\n",
    "        plt.plot(larray, label=\"Training\")\n",
    "        # plt.semilogy(ltarray, label=\"Test\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(_filename(f\"training_loss.png\"))\n",
    "\n",
    "fig, axs = panel(1, 1)\n",
    "# plt.semilogy(larray, label=\"Training\")\n",
    "plt.plot(larray, label=\"Training\")\n",
    "# plt.semilogy(ltarray, label=\"Test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(_filename(f\"training_loss.png\"))\n",
    "\n",
    "params = get_params(opt_state)\n",
    "savefile(f\"fgnode_trained_model.dil\", params, metadata=metadata)\n",
    "# savefile(f\"loss_array.dil\", (larray, ltarray), metadata=metadata)\n",
    "\n",
    "if last_loss > larray[-1]:\n",
    "    last_loss = larray[-1]\n",
    "    savefile(f\"fgnode_trained_model_low.dil\", params, metadata=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params, _ = loadfile(f\"fgnode_trained_model_low.dil\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rname=False\n",
    "\n",
    "# PSYS = f\"a-{N}-Spring-data-brownian_EM\"\n",
    "# TAG = f\"1NN\"\n",
    "# out_dir = f\"../results\"\n",
    "\n",
    "def _filename(name, tag=TAG):\n",
    "    rstring = randfilename if (rname and (tag != \"data\")) else (\n",
    "        \"0\" if (tag == \"data\") or (withdata == None) else f\"0_{withdata}\")\n",
    "    filename_prefix = f\"{out_dir}/{PSYS}-{tag}/{rstring}/\"\n",
    "    file = f\"{filename_prefix}/{name}\"\n",
    "    os.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "    filename = f\"{filename_prefix}/{name}\".replace(\"//\", \"/\")\n",
    "    print(\"===\", filename, \"===\")\n",
    "    return filename\n",
    "\n",
    "def OUT(f):\n",
    "    @wraps(f)\n",
    "    def func(file, *args, tag=TAG, **kwargs):\n",
    "        return f(_filename(file, tag=tag), *args, **kwargs)\n",
    "    return func\n",
    "\n",
    "loadmodel = OUT(src.models.loadmodel)\n",
    "savemodel = OUT(src.models.savemodel)\n",
    "\n",
    "loadfile = OUT(src.io.loadfile)\n",
    "savefile = OUT(src.io.savefile)\n",
    "save_ovito = OUT(src.io.save_ovito)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ../results/a-5-Spring-data-brownian_EM-1NN/0/fgnode_trained_model_low.dil ===\n",
      "Loading ../results/a-5-Spring-data-brownian_EM-1NN/0/fgnode_trained_model_low.dil\n"
     ]
    }
   ],
   "source": [
    "params, _ = loadfile(f\"fgnode_trained_model_low.dil\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "spring_constant = 1.0\n",
    "length_constant = 1.0\n",
    "# gamma_orig = jnp.ones(jnp.unique(species).shape)\n",
    "gamma_orig = jnp.where(jnp.arange(N) <3, 1.0, 2.0).reshape(-1,1)\n",
    "stride = 1\n",
    "runs=100\n",
    "\n",
    "def SPRING(x, stiffness=1.0, length=1.0):\n",
    "    x_ = jnp.linalg.norm(x, keepdims=True)\n",
    "    return 0.5*stiffness*(x_ - length)**2\n",
    "\n",
    "def pot_energy_orig(x):\n",
    "    dr = x[senders, :] - x[receivers, :]\n",
    "    return vmap(partial(SPRING, stiffness=spring_constant, length=length_constant))(dr).sum()\n",
    "\n",
    "def force_fn_orig(R, params):\n",
    "    return -grad(pot_energy_orig)(R)\n",
    "\n",
    "def get_forward_sim(params = None, force_fn = None, gamma = None, runs=10):\n",
    "        @jit\n",
    "        def fn(R,key):\n",
    "            return predition_brow(R, params, force_fn, shift, dt, kT, masses, gamma = gamma, stride=stride, runs=runs, key=key)\n",
    "        return fn\n",
    "\n",
    "\n",
    "sim_orig = get_forward_sim(params=None,force_fn=force_fn_orig, gamma=gamma_orig,runs=runs)\n",
    "\n",
    "# model\n",
    "def get_forward_sim_model(params = None, next_step_fn = None, runs=10, stride=1):\n",
    "        next_step_fn = lambda R: next_step_fn_model(R, params)\n",
    "        @jit\n",
    "        def solve_dynamics(R_init):\n",
    "            step = jit(lambda i, R: next_step_fn(R))\n",
    "            def f(R):\n",
    "                y = jax.lax.fori_loop(0, stride, step, R)\n",
    "                return y, y\n",
    "            \n",
    "            def func(R, i): return f(R)\n",
    "            @jit\n",
    "            def scan(R0):\n",
    "                return jax.lax.scan(func, R0, jnp.array(range(runs)))\n",
    "            \n",
    "            final_state, traj = scan(R_init)\n",
    "            return traj\n",
    "        return solve_dynamics\n",
    "\n",
    "sim_model = get_forward_sim_model(params = params, next_step_fn = next_step_fn_model, runs=runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_xyz_traj(Filepath,Name,R):\n",
    "    '''Writes ovito xyz file'''\n",
    "    f=open(Filepath,'w')\n",
    "    f.write(str(R.shape[1])+\"\\n\")\n",
    "    f.write(Name)\n",
    "    for i in range(R.shape[0]): #R.shape[0]\n",
    "        for j in range(R.shape[1]):\n",
    "            f.write(\"\\n\"+str(species[j])+\"\\t\"+str(R[i,j,0])+\"\\t\"+str(R[i,j,1])+\"\\t\"+str(R[i,j,2]))\n",
    "        f.write(\"\\n\"+str(R.shape[1]))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating trajectory 0/100 ...\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/actual_0_0.xyz ===\n",
      "Saving ovito file: ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/actual_0_0.xyz\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/pred_0_0.xyz ===\n",
      "Simulating trajectory 1/100 ...\n",
      "Simulating trajectory 2/100 ...\n",
      "Simulating trajectory 3/100 ...\n",
      "Simulating trajectory 4/100 ...\n",
      "Simulating trajectory 5/100 ...\n",
      "Simulating trajectory 6/100 ...\n",
      "Simulating trajectory 7/100 ...\n",
      "Simulating trajectory 8/100 ...\n",
      "Simulating trajectory 9/100 ...\n",
      "Simulating trajectory 10/100 ...\n",
      "Simulating trajectory 11/100 ...\n",
      "Simulating trajectory 12/100 ...\n",
      "Simulating trajectory 13/100 ...\n",
      "Simulating trajectory 14/100 ...\n",
      "Simulating trajectory 15/100 ...\n",
      "Simulating trajectory 16/100 ...\n",
      "Simulating trajectory 17/100 ...\n",
      "Simulating trajectory 18/100 ...\n",
      "Simulating trajectory 19/100 ...\n",
      "Simulating trajectory 20/100 ...\n",
      "Simulating trajectory 21/100 ...\n",
      "Simulating trajectory 22/100 ...\n",
      "Simulating trajectory 23/100 ...\n",
      "Simulating trajectory 24/100 ...\n",
      "Simulating trajectory 25/100 ...\n",
      "Simulating trajectory 26/100 ...\n",
      "Simulating trajectory 27/100 ...\n",
      "Simulating trajectory 28/100 ...\n",
      "Simulating trajectory 29/100 ...\n",
      "Simulating trajectory 30/100 ...\n",
      "Simulating trajectory 31/100 ...\n",
      "Simulating trajectory 32/100 ...\n",
      "Simulating trajectory 33/100 ...\n",
      "Simulating trajectory 34/100 ...\n",
      "Simulating trajectory 35/100 ...\n",
      "Simulating trajectory 36/100 ...\n",
      "Simulating trajectory 37/100 ...\n",
      "Simulating trajectory 38/100 ...\n",
      "Simulating trajectory 39/100 ...\n",
      "Simulating trajectory 40/100 ...\n",
      "Simulating trajectory 41/100 ...\n",
      "Simulating trajectory 42/100 ...\n",
      "Simulating trajectory 43/100 ...\n",
      "Simulating trajectory 44/100 ...\n",
      "Simulating trajectory 45/100 ...\n",
      "Simulating trajectory 46/100 ...\n",
      "Simulating trajectory 47/100 ...\n",
      "Simulating trajectory 48/100 ...\n",
      "Simulating trajectory 49/100 ...\n",
      "Simulating trajectory 50/100 ...\n",
      "Simulating trajectory 51/100 ...\n",
      "Simulating trajectory 52/100 ...\n",
      "Simulating trajectory 53/100 ...\n",
      "Simulating trajectory 54/100 ...\n",
      "Simulating trajectory 55/100 ...\n",
      "Simulating trajectory 56/100 ...\n",
      "Simulating trajectory 57/100 ...\n",
      "Simulating trajectory 58/100 ...\n",
      "Simulating trajectory 59/100 ...\n",
      "Simulating trajectory 60/100 ...\n",
      "Simulating trajectory 61/100 ...\n",
      "Simulating trajectory 62/100 ...\n",
      "Simulating trajectory 63/100 ...\n",
      "Simulating trajectory 64/100 ...\n",
      "Simulating trajectory 65/100 ...\n",
      "Simulating trajectory 66/100 ...\n",
      "Simulating trajectory 67/100 ...\n",
      "Simulating trajectory 68/100 ...\n",
      "Simulating trajectory 69/100 ...\n",
      "Simulating trajectory 70/100 ...\n",
      "Simulating trajectory 71/100 ...\n",
      "Simulating trajectory 72/100 ...\n",
      "Simulating trajectory 73/100 ...\n",
      "Simulating trajectory 74/100 ...\n",
      "Simulating trajectory 75/100 ...\n",
      "Simulating trajectory 76/100 ...\n",
      "Simulating trajectory 77/100 ...\n",
      "Simulating trajectory 78/100 ...\n",
      "Simulating trajectory 79/100 ...\n",
      "Simulating trajectory 80/100 ...\n",
      "Simulating trajectory 81/100 ...\n",
      "Simulating trajectory 82/100 ...\n",
      "Simulating trajectory 83/100 ...\n",
      "Simulating trajectory 84/100 ...\n",
      "Simulating trajectory 85/100 ...\n",
      "Simulating trajectory 86/100 ...\n",
      "Simulating trajectory 87/100 ...\n",
      "Simulating trajectory 88/100 ...\n",
      "Simulating trajectory 89/100 ...\n",
      "Simulating trajectory 90/100 ...\n",
      "Simulating trajectory 91/100 ...\n",
      "Simulating trajectory 92/100 ...\n",
      "Simulating trajectory 93/100 ...\n",
      "Simulating trajectory 94/100 ...\n",
      "Simulating trajectory 95/100 ...\n",
      "Simulating trajectory 96/100 ...\n",
      "Simulating trajectory 97/100 ...\n",
      "Simulating trajectory 98/100 ...\n",
      "Simulating trajectory 99/100 ...\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/error_paramete_plot_a_b_c.pkl ===\n",
      "=== ../results/a-10-AB-Spring-data-brownian_EM-1NN/05-24-2023_22-14-31/error_parameter.pkl ===\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# plotthings = True\n",
    "rng_key = random.PRNGKey(0)\n",
    "maxtraj = 100\n",
    "np.random.seed(seed)\n",
    "key = random.PRNGKey(seed)\n",
    "\n",
    "nexp = {\n",
    "        \"dz_actual\": [],\n",
    "        \"dz_pred\": [],\n",
    "        \"z_actual\": [],\n",
    "        \"z_pred\": [],\n",
    "        \"_gamma\": [],\n",
    "        \"simulation_time\":[],\n",
    "        }\n",
    "\n",
    "trajectories = []\n",
    "for ind in range(maxtraj):\n",
    "    print(f\"Simulating trajectory {ind}/{maxtraj} ...\")\n",
    "    R, _ = chain(N)[:2]\n",
    "    for rand in range(10):\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "        actual_traj = sim_orig(R,(ind+13)*subkey)\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "        \n",
    "        start = time.time()\n",
    "        pred_pos = sim_model(R)\n",
    "        end = time.time()\n",
    "        nexp[\"simulation_time\"] += [end-start]\n",
    "        \n",
    "        nexp[\"dz_actual\"] += [actual_traj.position-R]\n",
    "        nexp[\"dz_pred\"] += [pred_pos-R]\n",
    "\n",
    "        nexp[\"z_actual\"] += [actual_traj.position]\n",
    "        nexp[\"z_pred\"] += [pred_pos]\n",
    "        \n",
    "        if save_ovito:\n",
    "            if ind<1 and rand<1:\n",
    "                save_ovito(f\"actual_{ind}_{rand}.xyz\", [state for state in BrownianStates(actual_traj)], lattice=\"\")\n",
    "                write_xyz_traj(_filename(f\"pred_{ind}_{rand}.xyz\"),'spring_ddnn',pred_pos)\n",
    "                \n",
    "        # trajectories += [(actual_traj.position, pred_pos)]\n",
    "        # if ind%10==0:\n",
    "        #     savefile(\"trajectories.pkl\", trajectories)\n",
    "\n",
    "\n",
    "def KL_divergence(sigma0,mu0,sigma1,mu1, eps=1e-8):\n",
    "    return jnp.log(sigma1/sigma0) + (jnp.square(sigma0)+jnp.square(mu0-mu1))/(2*jnp.square(sigma1)) - 0.5\n",
    "\n",
    "def get_kld(d_actual, d_pred):\n",
    "    mu0 = jnp.mean(d_actual, axis=(0,2,3))\n",
    "    std0 = jnp.std(d_actual, axis=(0,2,3))\n",
    "    mu1 = jnp.mean(d_pred, axis=(0,2,3))\n",
    "    std1 = jnp.std(d_pred, axis=(0,2,3))\n",
    "    kld = []\n",
    "    for i in range(len(std0)):\n",
    "        kld.append(KL_divergence(std0[i],mu0[i],std1[i],mu1[i]))\n",
    "    return jnp.array(kld)\n",
    "\n",
    "def get_std_rmse(d_actual, d_pred):\n",
    "    std0 = jnp.std(d_actual, axis=(0,2,3))\n",
    "    std1 = jnp.std(d_pred, axis=(0,2,3))\n",
    "    return jnp.sqrt(jnp.square(std0 - std1))\n",
    "\n",
    "def get_dist_by_var(actual, pred, zeta):\n",
    "    disp = displacement(actual, pred)\n",
    "    dist_matrix = jnp.sqrt(jnp.square(disp).sum(-1))\n",
    "    dist_mean = jnp.mean(dist_matrix, axis=(0,2))\n",
    "    dist_by_zeta = dist_mean/zeta\n",
    "    return dist_by_zeta\n",
    "\n",
    "nexp2 = {\n",
    "        \"kld\": [],\n",
    "        \"std_rmse\": [],\n",
    "        }\n",
    "\n",
    "nexp2[\"kld\"] = jnp.array(get_kld(jnp.array(nexp[\"dz_actual\"]),jnp.array(nexp[\"dz_pred\"])))\n",
    "\n",
    "nexp2[\"std_rmse\"] = jnp.array(get_std_rmse(jnp.array(nexp[\"dz_actual\"]),jnp.array(nexp[\"dz_pred\"])))\n",
    "\n",
    "# nexp2[\"dist_by_var\"] = jnp.array(get_dist_by_var(jnp.array(nexp['z_actual']), jnp.array(nexp['z_pred']),1/(jnp.array(nexp['_gamma'])[0][0])))\n",
    "\n",
    "savefile(f\"error_paramete_plot_a_b_c.pkl\", nexp2)\n",
    "# savefile(\"trajectories.pkl\", trajectories)\n",
    "savefile(f\"error_parameter.pkl\", nexp)\n",
    "# savefile(\"trajectories.pkl\", trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ../results/a-5-Spring-data-brownian_EM-NN/0/mu_sigma.png ===\n",
      "=== ../results/a-5-Spring-data-brownian_EM-NN/0/kld1.png ===\n",
      "=== ../results/a-5-Spring-data-brownian_EM-NN/0/kld_x_y.png ===\n"
     ]
    }
   ],
   "source": [
    "def KL_divergence(sigma0,mu0,sigma1,mu1, eps=1e-8):\n",
    "    # q = np.where(q == 0.0, eps, q)\n",
    "    # p = np.where(p == 0, eps, p)\n",
    "    return jnp.log(sigma1/sigma0) + (jnp.square(sigma0)+jnp.square(mu0-mu1))/(2*jnp.square(sigma1)) - 0.5\n",
    "\n",
    "actual = jnp.array(nexp[\"dz_actual\"])\n",
    "pred = jnp.array(nexp[\"dz_pred\"])\n",
    "\n",
    "\n",
    "mu0 = jnp.mean(actual, axis=(0,2,3))\n",
    "std0 = jnp.std(actual, axis=(0,2,3))\n",
    "\n",
    "mu1 = jnp.mean(pred, axis=(0,2,3))\n",
    "std1 = jnp.std(pred, axis=(0,2,3))\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(mu0,label='mu0')\n",
    "plt.plot(mu1,label='mu1')\n",
    "plt.plot(std0,label='std0')\n",
    "plt.plot(std1,label='std1')\n",
    "plt.legend()\n",
    "plt.savefig(_filename('mu_sigma.png'))\n",
    "plt.clf()\n",
    "\n",
    "kld = []\n",
    "for i in range(100):\n",
    "    kld.append(KL_divergence(std0[i],mu0[i],std1[i],mu1[i]))\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(kld)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('$D_{KL}$($\\hat{X}$||X)')\n",
    "plt.savefig(_filename('kld1.png'))\n",
    "plt.clf()\n",
    "\n",
    "mu0 = jnp.mean(actual, axis=(0,2))\n",
    "std0 = jnp.std(actual, axis=(0,2))\n",
    "\n",
    "mu1 = jnp.mean(pred, axis=(0,2))\n",
    "std1 = jnp.std(pred, axis=(0,2))\n",
    "\n",
    "kld_x = []\n",
    "for i in range(100):\n",
    "    kld_x.append(KL_divergence(std0[i,0],mu0[i,0],std1[i,0],mu1[i,0]))\n",
    "\n",
    "kld_y = []\n",
    "for i in range(100):\n",
    "    kld_y.append(KL_divergence(std0[i,1],mu0[i,1],std1[i,1],mu1[i,1]))\n",
    "\n",
    "\n",
    "plt.plot(kld_x, label ='x')\n",
    "plt.plot(kld_y, label ='y')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('$D_{KL}$($\\hat{X}$||X)')\n",
    "plt.legend()\n",
    "plt.savefig(_filename('kld_x_y.png'))\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_jaxbrow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
