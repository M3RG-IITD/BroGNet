{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## IMPORT ######################\n",
    "import json\n",
    "from datetime import datetime\n",
    "from functools import partial, wraps\n",
    "\n",
    "import fire\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, random, value_and_grad, vmap\n",
    "# from jax.experimental import optimizers\n",
    "from jax.example_libraries import optimizers\n",
    "from jax_md import space\n",
    "from shadow.plot import *\n",
    "from sklearn.metrics import r2_score\n",
    "from psystems.nsprings import (chain, edge_order, get_connections,\n",
    "                               get_fully_connected_senders_and_receivers,\n",
    "                               get_fully_edge_order)\n",
    "# from statistics import mode\n",
    "# from sympy import LM\n",
    "# from torch import batch_norm_gather_stats_with_counts\n",
    "import sys\n",
    "MAINPATH = \"..\"  # nopep8\n",
    "sys.path.append(MAINPATH)  # nopep8\n",
    "import jraph\n",
    "import src\n",
    "from jax.config import config\n",
    "# from src import fgn, lnn\n",
    "from src.graph import *\n",
    "# from src.lnn import acceleration, accelerationFull, accelerationTV\n",
    "from src.md import *\n",
    "from src.models import MSE, initialize_mlp, GaussianNLL, initialize_mlp_gamma, forward_pass_gamma\n",
    "from src.nve import NVEStates, nve, BrownianStates\n",
    "from src.utils import *\n",
    "\n",
    "# config.update(\"jax_enable_x64\", True)\n",
    "# config.update(\"jax_debug_nans\", True)\n",
    "# jax.config.update('jax_platform_name', 'gpu')\n",
    "\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "\n",
    "def pprint(*args, namespace=globals()):\n",
    "    for arg in args:\n",
    "        print(f\"{namestr(arg, namespace)[0]}: {arg}\")\n",
    "\n",
    "f32 = jnp.float32\n",
    "f64 = jnp.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs: \n",
      "N: 5\n",
      "epochs: 10000\n",
      "seed: 42\n",
      "rname: True\n",
      "dt: 0.001\n",
      "lr: 0.0001\n",
      "batch_size: 20\n"
     ]
    }
   ],
   "source": [
    "N = 5  # number of particles\n",
    "dim = 2  # dimensions\n",
    "runs = 1\n",
    "kT = 1 #1.380649e-23*T  # boltzmann constant*temperature\n",
    "# spring_constant = 1.0\n",
    "# length_constant = 1.0\n",
    "# nconfig=100\n",
    "seed=42\n",
    "dt = 1.0e-3 # time step*stride \n",
    "lr=1e-4\n",
    "batch_size=20\n",
    "epochs = 10000\n",
    "# node_type = jnp.array([0,0,0,0,0])\n",
    "masses = jnp.ones(N)\n",
    "species = jnp.zeros(N, dtype=int).reshape(-1,1)\n",
    "# gamma = jnp.ones(jnp.unique(species).shape)  # damping constant\n",
    "\n",
    "rname=True\n",
    "withdata = None\n",
    "\n",
    "print(\"Configs: \")\n",
    "pprint(N, epochs, seed, rname, dt, lr, batch_size, namespace=locals())\n",
    "\n",
    "randfilename = datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "\n",
    "PSYS = f\"a-{N}-non-linear-Spring-data-brownian_EM\"\n",
    "TAG = f\"2BNN\"\n",
    "out_dir = f\"../results\"\n",
    "\n",
    "def _filename(name, tag=TAG):\n",
    "    rstring = randfilename if (rname and (tag != \"data\")) else (\n",
    "        \"0\" if (tag == \"data\") or (withdata == None) else f\"0_{withdata}\")\n",
    "    filename_prefix = f\"{out_dir}/{PSYS}-{tag}/{rstring}/\"\n",
    "    file = f\"{filename_prefix}/{name}\"\n",
    "    os.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "    filename = f\"{filename_prefix}/{name}\".replace(\"//\", \"/\")\n",
    "    print(\"===\", filename, \"===\")\n",
    "    return filename\n",
    "\n",
    "def displacement(a, b):\n",
    "    return a - b\n",
    "\n",
    "def shift(R, dR):\n",
    "    return R+dR\n",
    "\n",
    "def OUT(f):\n",
    "    @wraps(f)\n",
    "    def func(file, *args, tag=TAG, **kwargs):\n",
    "        return f(_filename(file, tag=tag), *args, **kwargs)\n",
    "    return func\n",
    "\n",
    "loadmodel = OUT(src.models.loadmodel)\n",
    "savemodel = OUT(src.models.savemodel)\n",
    "\n",
    "loadfile = OUT(src.io.loadfile)\n",
    "savefile = OUT(src.io.savefile)\n",
    "save_ovito = OUT(src.io.save_ovito)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-data/0/model_states_brownian.pkl ===\n",
      "Total number of data points: 100x100\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "################## CONFIG ######################\n",
    "################################################\n",
    "np.random.seed(seed)\n",
    "key = random.PRNGKey(seed)\n",
    "\n",
    "try:\n",
    "    dataset_states = loadfile(f\"model_states_brownian.pkl\", tag=\"data\")[0]\n",
    "except:\n",
    "    raise Exception(\"Generate dataset first.\")\n",
    "\n",
    "model_states = dataset_states[0]\n",
    "\n",
    "print(f\"Total number of data points: {len(dataset_states)}x{model_states.position.shape[0]}\")\n",
    "\n",
    "Rs = States_Brow().fromlist(dataset_states).get_array()\n",
    "\n",
    "Rs_in = Rs[:,:99,:,:]\n",
    "Rs_out = Rs[:,1:100,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "################### ML Model ###################\n",
    "################################################\n",
    "# print(\"Creating Chain\")\n",
    "x, _, senders, receivers = chain(N)\n",
    "\n",
    "hidden = 16\n",
    "nhidden = 2\n",
    "\n",
    "def get_layers(in_, out_):\n",
    "    return [in_] + [hidden]*nhidden + [out_]\n",
    "\n",
    "def mlp(in_, out_, key, **kwargs):\n",
    "    return initialize_mlp(get_layers(in_, out_), key, **kwargs)\n",
    "\n",
    "params = {\"F_pos\": mlp(N*dim, N*dim, key)}\n",
    "\n",
    "params[\"gamma\"] = initialize_mlp_gamma([1,10,5,1], key)\n",
    "\n",
    "def nngamma(type, params):\n",
    "    return forward_pass_gamma(params, type, activation_fn=models.SquarePlus)\n",
    "\n",
    "def gamma(type, params):\n",
    "    return vmap(nngamma, in_axes=(0, None))(type.reshape(-1), params).reshape(-1, 1)\n",
    "    # return nngamma(type.reshape(-1), params[\"gamma\"])#.reshape(-1, 1)\n",
    "\n",
    "# ss = gamma(jax.nn.one_hot(species, 1),params[\"gamma\"])\n",
    "\n",
    "\n",
    "def acceleration_node(x, params, **kwargs):\n",
    "    n,dim = x.shape\n",
    "    inp = x.flatten() #jnp.hstack([x.flatten(),v.flatten()])\n",
    "    out = forward_pass(params, inp)\n",
    "    return out.reshape(-1,dim)\n",
    "\n",
    "def _force_fn():    \n",
    "    def apply(R, params):\n",
    "        return acceleration_node(R, params)\n",
    "    return apply\n",
    "\n",
    "def gamma_fn(species):    \n",
    "    def fn(params):\n",
    "        return gamma(jax.nn.one_hot(species, 1),params)    \n",
    "    return fn\n",
    "\n",
    "apply_fn = _force_fn()\n",
    "gamma_fn = gamma_fn(species)\n",
    "\n",
    "def force_fn_model(x, params): return apply_fn(x, params['F_pos'])\n",
    "def gamma_fn_model(params): return gamma_fn(params[\"gamma\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step_pos_gamma(force_fn_model, gamma_fn_model, shift, dt, kT, mass, runs, key):\n",
    "    key, split = random.split(key)\n",
    "    def fn(x, params):\n",
    "        for i in range(runs):\n",
    "            # calculate the force\n",
    "            force = force_fn_model(x, params)\n",
    "            _gamma = gamma_fn_model(params)\n",
    "            xi = random.normal(split, x.shape, x.dtype)\n",
    "            nu = f32(1) / lax.mul(mass.reshape(-1,1) , _gamma)\n",
    "            x = x+ force * dt * nu + jnp.sqrt(f32(2) * kT * dt * nu) * xi\n",
    "        return x, _gamma\n",
    "    return fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = random.PRNGKey(0)\n",
    "rng_key, subkey = random.split(rng_key)\n",
    "\n",
    "next_step_pos_gamma_fn = next_step_pos_gamma(force_fn_model, gamma_fn_model, shift, dt, kT, masses, runs, subkey)\n",
    "v_next_step_pos_gamma_fn = vmap(next_step_pos_gamma_fn, in_axes=(0, None))\n",
    "v_v_next_step_pos_gamma_fn = vmap(v_next_step_pos_gamma_fn, in_axes=(0, None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ...\n",
      "Epoch: 0/10000 Loss (MSE):  train=4.589895248413086\n",
      "gammaaaaa:  [[5.229261]\n",
      " [5.229261]\n",
      " [5.229261]\n",
      " [5.229261]\n",
      " [5.229261]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 10/10000 Loss (MSE):  train=4.466156959533691\n",
      "Epoch: 20/10000 Loss (MSE):  train=4.345693111419678\n",
      "Epoch: 30/10000 Loss (MSE):  train=4.228562355041504\n",
      "Epoch: 40/10000 Loss (MSE):  train=4.114636421203613\n",
      "Epoch: 50/10000 Loss (MSE):  train=4.003787994384766\n",
      "Epoch: 60/10000 Loss (MSE):  train=3.8958611488342285\n",
      "Epoch: 70/10000 Loss (MSE):  train=3.7907114028930664\n",
      "Epoch: 80/10000 Loss (MSE):  train=3.688192844390869\n",
      "Epoch: 90/10000 Loss (MSE):  train=3.588174819946289\n",
      "Epoch: 100/10000 Loss (MSE):  train=3.490536689758301\n",
      "gammaaaaa:  [[3.8990664]\n",
      " [3.8990664]\n",
      " [3.8990664]\n",
      " [3.8990664]\n",
      " [3.8990664]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 110/10000 Loss (MSE):  train=3.395145893096924\n",
      "Epoch: 120/10000 Loss (MSE):  train=3.3018739223480225\n",
      "Epoch: 130/10000 Loss (MSE):  train=3.2105860710144043\n",
      "Epoch: 140/10000 Loss (MSE):  train=3.121140718460083\n",
      "Epoch: 150/10000 Loss (MSE):  train=3.033400535583496\n",
      "Epoch: 160/10000 Loss (MSE):  train=2.94722056388855\n",
      "Epoch: 170/10000 Loss (MSE):  train=2.8624682426452637\n",
      "Epoch: 180/10000 Loss (MSE):  train=2.7790024280548096\n",
      "Epoch: 190/10000 Loss (MSE):  train=2.6966989040374756\n",
      "Epoch: 200/10000 Loss (MSE):  train=2.6154372692108154\n",
      "gammaaaaa:  [[2.8309808]\n",
      " [2.8309808]\n",
      " [2.8309808]\n",
      " [2.8309808]\n",
      " [2.8309808]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 210/10000 Loss (MSE):  train=2.5351219177246094\n",
      "Epoch: 220/10000 Loss (MSE):  train=2.455665349960327\n",
      "Epoch: 230/10000 Loss (MSE):  train=2.3770201206207275\n",
      "Epoch: 240/10000 Loss (MSE):  train=2.2991628646850586\n",
      "Epoch: 250/10000 Loss (MSE):  train=2.222107410430908\n",
      "Epoch: 260/10000 Loss (MSE):  train=2.145900249481201\n",
      "Epoch: 270/10000 Loss (MSE):  train=2.070634603500366\n",
      "Epoch: 280/10000 Loss (MSE):  train=1.99643874168396\n",
      "Epoch: 290/10000 Loss (MSE):  train=1.9234923124313354\n",
      "Epoch: 300/10000 Loss (MSE):  train=1.8520241975784302\n",
      "gammaaaaa:  [[1.9589046]\n",
      " [1.9589046]\n",
      " [1.9589046]\n",
      " [1.9589046]\n",
      " [1.9589046]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 310/10000 Loss (MSE):  train=1.7823207378387451\n",
      "Epoch: 320/10000 Loss (MSE):  train=1.7147135734558105\n",
      "Epoch: 330/10000 Loss (MSE):  train=1.6495747566223145\n",
      "Epoch: 340/10000 Loss (MSE):  train=1.58729887008667\n",
      "Epoch: 350/10000 Loss (MSE):  train=1.528287410736084\n",
      "Epoch: 360/10000 Loss (MSE):  train=1.4729223251342773\n",
      "Epoch: 370/10000 Loss (MSE):  train=1.4215482473373413\n",
      "Epoch: 380/10000 Loss (MSE):  train=1.3744292259216309\n",
      "Epoch: 390/10000 Loss (MSE):  train=1.3317269086837769\n",
      "Epoch: 400/10000 Loss (MSE):  train=1.2934682369232178\n",
      "gammaaaaa:  [[1.3337303]\n",
      " [1.3337303]\n",
      " [1.3337303]\n",
      " [1.3337303]\n",
      " [1.3337303]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 410/10000 Loss (MSE):  train=1.2595396041870117\n",
      "Epoch: 420/10000 Loss (MSE):  train=1.2296957969665527\n",
      "Epoch: 430/10000 Loss (MSE):  train=1.2035959959030151\n",
      "Epoch: 440/10000 Loss (MSE):  train=1.180849313735962\n",
      "Epoch: 450/10000 Loss (MSE):  train=1.1610493659973145\n",
      "Epoch: 460/10000 Loss (MSE):  train=1.143814206123352\n",
      "Epoch: 470/10000 Loss (MSE):  train=1.1287949085235596\n",
      "Epoch: 480/10000 Loss (MSE):  train=1.1156859397888184\n",
      "Epoch: 490/10000 Loss (MSE):  train=1.1042245626449585\n",
      "Epoch: 500/10000 Loss (MSE):  train=1.094185709953308\n",
      "gammaaaaa:  [[1.0521009]\n",
      " [1.0521009]\n",
      " [1.0521009]\n",
      " [1.0521009]\n",
      " [1.0521008]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 510/10000 Loss (MSE):  train=1.085376262664795\n",
      "Epoch: 520/10000 Loss (MSE):  train=1.077638864517212\n",
      "Epoch: 530/10000 Loss (MSE):  train=1.0708421468734741\n",
      "Epoch: 540/10000 Loss (MSE):  train=1.0648796558380127\n",
      "Epoch: 550/10000 Loss (MSE):  train=1.059656023979187\n",
      "Epoch: 560/10000 Loss (MSE):  train=1.0550885200500488\n",
      "Epoch: 570/10000 Loss (MSE):  train=1.0511022806167603\n",
      "Epoch: 580/10000 Loss (MSE):  train=1.0476282835006714\n",
      "Epoch: 590/10000 Loss (MSE):  train=1.0446054935455322\n",
      "Epoch: 600/10000 Loss (MSE):  train=1.0419777631759644\n",
      "gammaaaaa:  [[0.98014903]\n",
      " [0.98014903]\n",
      " [0.98014903]\n",
      " [0.98014903]\n",
      " [0.98014903]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 610/10000 Loss (MSE):  train=1.0396959781646729\n",
      "Epoch: 620/10000 Loss (MSE):  train=1.0377137660980225\n",
      "Epoch: 630/10000 Loss (MSE):  train=1.0359944105148315\n",
      "Epoch: 640/10000 Loss (MSE):  train=1.0344998836517334\n",
      "Epoch: 650/10000 Loss (MSE):  train=1.0332025289535522\n",
      "Epoch: 660/10000 Loss (MSE):  train=1.032075047492981\n",
      "Epoch: 670/10000 Loss (MSE):  train=1.0310946702957153\n",
      "Epoch: 680/10000 Loss (MSE):  train=1.0302414894104004\n",
      "Epoch: 690/10000 Loss (MSE):  train=1.0294990539550781\n",
      "Epoch: 700/10000 Loss (MSE):  train=1.0288516283035278\n",
      "gammaaaaa:  [[0.97662854]\n",
      " [0.97662854]\n",
      " [0.97662854]\n",
      " [0.97662854]\n",
      " [0.9766285 ]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 710/10000 Loss (MSE):  train=1.0282855033874512\n",
      "Epoch: 720/10000 Loss (MSE):  train=1.0277893543243408\n",
      "Epoch: 730/10000 Loss (MSE):  train=1.027355432510376\n",
      "Epoch: 740/10000 Loss (MSE):  train=1.0269722938537598\n",
      "Epoch: 750/10000 Loss (MSE):  train=1.0266339778900146\n",
      "Epoch: 760/10000 Loss (MSE):  train=1.0263336896896362\n",
      "Epoch: 770/10000 Loss (MSE):  train=1.0260659456253052\n",
      "Epoch: 780/10000 Loss (MSE):  train=1.0258257389068604\n",
      "Epoch: 790/10000 Loss (MSE):  train=1.025608777999878\n",
      "Epoch: 800/10000 Loss (MSE):  train=1.0254113674163818\n",
      "gammaaaaa:  [[0.98569846]\n",
      " [0.98569846]\n",
      " [0.98569846]\n",
      " [0.98569846]\n",
      " [0.98569846]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 810/10000 Loss (MSE):  train=1.025230050086975\n",
      "Epoch: 820/10000 Loss (MSE):  train=1.0250623226165771\n",
      "Epoch: 830/10000 Loss (MSE):  train=1.0249077081680298\n",
      "Epoch: 840/10000 Loss (MSE):  train=1.0247607231140137\n",
      "Epoch: 850/10000 Loss (MSE):  train=1.0246217250823975\n",
      "Epoch: 860/10000 Loss (MSE):  train=1.0244886875152588\n",
      "Epoch: 870/10000 Loss (MSE):  train=1.0243619680404663\n",
      "Epoch: 880/10000 Loss (MSE):  train=1.024238109588623\n",
      "Epoch: 890/10000 Loss (MSE):  train=1.0241189002990723\n",
      "Epoch: 900/10000 Loss (MSE):  train=1.024001121520996\n",
      "gammaaaaa:  [[0.99229175]\n",
      " [0.99229175]\n",
      " [0.99229175]\n",
      " [0.99229175]\n",
      " [0.99229175]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 910/10000 Loss (MSE):  train=1.0238853693008423\n",
      "Epoch: 920/10000 Loss (MSE):  train=1.0237705707550049\n",
      "Epoch: 930/10000 Loss (MSE):  train=1.0236587524414062\n",
      "Epoch: 940/10000 Loss (MSE):  train=1.0235464572906494\n",
      "Epoch: 950/10000 Loss (MSE):  train=1.0234335660934448\n",
      "Epoch: 960/10000 Loss (MSE):  train=1.0233232975006104\n",
      "Epoch: 970/10000 Loss (MSE):  train=1.0232106447219849\n",
      "Epoch: 980/10000 Loss (MSE):  train=1.0231001377105713\n",
      "Epoch: 990/10000 Loss (MSE):  train=1.0229885578155518\n",
      "Epoch: 1000/10000 Loss (MSE):  train=1.022876501083374\n",
      "gammaaaaa:  [[0.9951303 ]\n",
      " [0.9951303 ]\n",
      " [0.9951303 ]\n",
      " [0.9951303 ]\n",
      " [0.99513024]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 1010/10000 Loss (MSE):  train=1.0227653980255127\n",
      "Epoch: 1020/10000 Loss (MSE):  train=1.0226528644561768\n",
      "Epoch: 1030/10000 Loss (MSE):  train=1.0225393772125244\n",
      "Epoch: 1040/10000 Loss (MSE):  train=1.0224270820617676\n",
      "Epoch: 1050/10000 Loss (MSE):  train=1.0223135948181152\n",
      "Epoch: 1060/10000 Loss (MSE):  train=1.022200584411621\n",
      "Epoch: 1070/10000 Loss (MSE):  train=1.022086501121521\n",
      "Epoch: 1080/10000 Loss (MSE):  train=1.0219714641571045\n",
      "Epoch: 1090/10000 Loss (MSE):  train=1.0218570232391357\n",
      "Epoch: 1100/10000 Loss (MSE):  train=1.0217413902282715\n",
      "gammaaaaa:  [[0.99599427]\n",
      " [0.99599427]\n",
      " [0.99599427]\n",
      " [0.99599427]\n",
      " [0.99599427]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 1110/10000 Loss (MSE):  train=1.0216255187988281\n",
      "Epoch: 1120/10000 Loss (MSE):  train=1.0215098857879639\n",
      "Epoch: 1130/10000 Loss (MSE):  train=1.0213932991027832\n",
      "Epoch: 1140/10000 Loss (MSE):  train=1.0212771892547607\n",
      "Epoch: 1150/10000 Loss (MSE):  train=1.021160364151001\n",
      "Epoch: 1160/10000 Loss (MSE):  train=1.021043062210083\n",
      "Epoch: 1170/10000 Loss (MSE):  train=1.0209264755249023\n",
      "Epoch: 1180/10000 Loss (MSE):  train=1.0208083391189575\n",
      "Epoch: 1190/10000 Loss (MSE):  train=1.0206904411315918\n",
      "Epoch: 1200/10000 Loss (MSE):  train=1.0205727815628052\n",
      "gammaaaaa:  [[0.9961787]\n",
      " [0.9961787]\n",
      " [0.9961787]\n",
      " [0.9961787]\n",
      " [0.9961787]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 1210/10000 Loss (MSE):  train=1.0204540491104126\n",
      "Epoch: 1220/10000 Loss (MSE):  train=1.0203357934951782\n",
      "Epoch: 1230/10000 Loss (MSE):  train=1.020216703414917\n",
      "Epoch: 1240/10000 Loss (MSE):  train=1.0200979709625244\n",
      "Epoch: 1250/10000 Loss (MSE):  train=1.0199792385101318\n",
      "Epoch: 1260/10000 Loss (MSE):  train=1.0198594331741333\n",
      "Epoch: 1270/10000 Loss (MSE):  train=1.0197404623031616\n",
      "Epoch: 1280/10000 Loss (MSE):  train=1.0196202993392944\n",
      "Epoch: 1290/10000 Loss (MSE):  train=1.0195014476776123\n",
      "Epoch: 1300/10000 Loss (MSE):  train=1.0193822383880615\n",
      "gammaaaaa:  [[0.99619186]\n",
      " [0.99619186]\n",
      " [0.99619186]\n",
      " [0.99619186]\n",
      " [0.99619186]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 1310/10000 Loss (MSE):  train=1.0192625522613525\n",
      "Epoch: 1320/10000 Loss (MSE):  train=1.0191428661346436\n",
      "Epoch: 1330/10000 Loss (MSE):  train=1.0190238952636719\n",
      "Epoch: 1340/10000 Loss (MSE):  train=1.018903374671936\n",
      "Epoch: 1350/10000 Loss (MSE):  train=1.0187835693359375\n",
      "Epoch: 1360/10000 Loss (MSE):  train=1.0186638832092285\n",
      "Epoch: 1370/10000 Loss (MSE):  train=1.0185444355010986\n",
      "Epoch: 1380/10000 Loss (MSE):  train=1.0184253454208374\n",
      "Epoch: 1390/10000 Loss (MSE):  train=1.0183053016662598\n",
      "Epoch: 1400/10000 Loss (MSE):  train=1.018186330795288\n",
      "gammaaaaa:  [[0.99616635]\n",
      " [0.99616635]\n",
      " [0.99616635]\n",
      " [0.99616635]\n",
      " [0.9961663 ]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 1410/10000 Loss (MSE):  train=1.0180662870407104\n",
      "Epoch: 1420/10000 Loss (MSE):  train=1.0179476737976074\n",
      "Epoch: 1430/10000 Loss (MSE):  train=1.0178275108337402\n",
      "Epoch: 1440/10000 Loss (MSE):  train=1.0177081823349\n",
      "Epoch: 1450/10000 Loss (MSE):  train=1.0175896883010864\n",
      "Epoch: 1460/10000 Loss (MSE):  train=1.017471194267273\n",
      "Epoch: 1470/10000 Loss (MSE):  train=1.01735258102417\n",
      "Epoch: 1480/10000 Loss (MSE):  train=1.0172340869903564\n",
      "Epoch: 1490/10000 Loss (MSE):  train=1.0171151161193848\n",
      "Epoch: 1500/10000 Loss (MSE):  train=1.0169973373413086\n",
      "gammaaaaa:  [[0.9961291 ]\n",
      " [0.9961291 ]\n",
      " [0.9961291 ]\n",
      " [0.9961291 ]\n",
      " [0.99612916]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 1510/10000 Loss (MSE):  train=1.016879677772522\n",
      "Epoch: 1520/10000 Loss (MSE):  train=1.0167615413665771\n",
      "Epoch: 1530/10000 Loss (MSE):  train=1.0166444778442383\n",
      "Epoch: 1540/10000 Loss (MSE):  train=1.0165272951126099\n",
      "Epoch: 1550/10000 Loss (MSE):  train=1.0164103507995605\n",
      "Epoch: 1560/10000 Loss (MSE):  train=1.0162937641143799\n",
      "Epoch: 1570/10000 Loss (MSE):  train=1.0161769390106201\n",
      "Epoch: 1580/10000 Loss (MSE):  train=1.0160614252090454\n",
      "Epoch: 1590/10000 Loss (MSE):  train=1.015945553779602\n",
      "Epoch: 1600/10000 Loss (MSE):  train=1.0158299207687378\n",
      "gammaaaaa:  [[0.9960828]\n",
      " [0.9960828]\n",
      " [0.9960828]\n",
      " [0.9960828]\n",
      " [0.9960828]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 1610/10000 Loss (MSE):  train=1.0157147645950317\n",
      "Epoch: 1620/10000 Loss (MSE):  train=1.0155994892120361\n",
      "Epoch: 1630/10000 Loss (MSE):  train=1.015485405921936\n",
      "Epoch: 1640/10000 Loss (MSE):  train=1.015371561050415\n",
      "Epoch: 1650/10000 Loss (MSE):  train=1.0152573585510254\n",
      "Epoch: 1660/10000 Loss (MSE):  train=1.0151437520980835\n",
      "Epoch: 1670/10000 Loss (MSE):  train=1.0150314569473267\n",
      "Epoch: 1680/10000 Loss (MSE):  train=1.0149190425872803\n",
      "Epoch: 1690/10000 Loss (MSE):  train=1.0148061513900757\n",
      "Epoch: 1700/10000 Loss (MSE):  train=1.0146945714950562\n",
      "gammaaaaa:  [[0.9960306]\n",
      " [0.9960306]\n",
      " [0.9960306]\n",
      " [0.9960306]\n",
      " [0.9960306]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 1710/10000 Loss (MSE):  train=1.0145829916000366\n",
      "Epoch: 1720/10000 Loss (MSE):  train=1.0144715309143066\n",
      "Epoch: 1730/10000 Loss (MSE):  train=1.0143612623214722\n",
      "Epoch: 1740/10000 Loss (MSE):  train=1.01425039768219\n",
      "Epoch: 1750/10000 Loss (MSE):  train=1.0141409635543823\n",
      "Epoch: 1760/10000 Loss (MSE):  train=1.014030933380127\n",
      "Epoch: 1770/10000 Loss (MSE):  train=1.013922095298767\n",
      "Epoch: 1780/10000 Loss (MSE):  train=1.013814091682434\n",
      "Epoch: 1790/10000 Loss (MSE):  train=1.0137054920196533\n",
      "Epoch: 1800/10000 Loss (MSE):  train=1.0135979652404785\n",
      "gammaaaaa:  [[0.9959798]\n",
      " [0.9959798]\n",
      " [0.9959798]\n",
      " [0.9959798]\n",
      " [0.9959797]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 1810/10000 Loss (MSE):  train=1.0134915113449097\n",
      "Epoch: 1820/10000 Loss (MSE):  train=1.0133839845657349\n",
      "Epoch: 1830/10000 Loss (MSE):  train=1.0132784843444824\n",
      "Epoch: 1840/10000 Loss (MSE):  train=1.0131726264953613\n",
      "Epoch: 1850/10000 Loss (MSE):  train=1.0130672454833984\n",
      "Epoch: 1860/10000 Loss (MSE):  train=1.0129623413085938\n",
      "Epoch: 1870/10000 Loss (MSE):  train=1.0128577947616577\n",
      "Epoch: 1880/10000 Loss (MSE):  train=1.0127537250518799\n",
      "Epoch: 1890/10000 Loss (MSE):  train=1.012650728225708\n",
      "Epoch: 1900/10000 Loss (MSE):  train=1.0125473737716675\n",
      "gammaaaaa:  [[0.99593776]\n",
      " [0.99593776]\n",
      " [0.99593776]\n",
      " [0.99593776]\n",
      " [0.99593776]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 1910/10000 Loss (MSE):  train=1.0124456882476807\n",
      "Epoch: 1920/10000 Loss (MSE):  train=1.012343406677246\n",
      "Epoch: 1930/10000 Loss (MSE):  train=1.0122421979904175\n",
      "Epoch: 1940/10000 Loss (MSE):  train=1.0121407508850098\n",
      "Epoch: 1950/10000 Loss (MSE):  train=1.012040138244629\n",
      "Epoch: 1960/10000 Loss (MSE):  train=1.0119398832321167\n",
      "Epoch: 1970/10000 Loss (MSE):  train=1.0118415355682373\n",
      "Epoch: 1980/10000 Loss (MSE):  train=1.0117418766021729\n",
      "Epoch: 1990/10000 Loss (MSE):  train=1.0116435289382935\n",
      "Epoch: 2000/10000 Loss (MSE):  train=1.0115457773208618\n",
      "gammaaaaa:  [[0.99590915]\n",
      " [0.99590915]\n",
      " [0.99590915]\n",
      " [0.99590915]\n",
      " [0.99590915]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sureshjyoti/GitHub/BGNODE_scratch/.venv_jaxbrow/lib/python3.9/site-packages/shadow/plot.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(figsize=figsize, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2010/10000 Loss (MSE):  train=1.0114483833312988\n",
      "Epoch: 2020/10000 Loss (MSE):  train=1.0113509893417358\n",
      "Epoch: 2030/10000 Loss (MSE):  train=1.011254072189331\n",
      "Epoch: 2040/10000 Loss (MSE):  train=1.0111576318740845\n",
      "Epoch: 2050/10000 Loss (MSE):  train=1.011062741279602\n",
      "Epoch: 2060/10000 Loss (MSE):  train=1.0109682083129883\n",
      "Epoch: 2070/10000 Loss (MSE):  train=1.0108734369277954\n",
      "Epoch: 2080/10000 Loss (MSE):  train=1.0107786655426025\n",
      "Epoch: 2090/10000 Loss (MSE):  train=1.010685682296753\n",
      "Epoch: 2100/10000 Loss (MSE):  train=1.010591745376587\n",
      "gammaaaaa:  [[0.9958973]\n",
      " [0.9958973]\n",
      " [0.9958973]\n",
      " [0.9958973]\n",
      " [0.9958974]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 2110/10000 Loss (MSE):  train=1.0104990005493164\n",
      "Epoch: 2120/10000 Loss (MSE):  train=1.0104070901870728\n",
      "Epoch: 2130/10000 Loss (MSE):  train=1.010316252708435\n",
      "Epoch: 2140/10000 Loss (MSE):  train=1.0102249383926392\n",
      "Epoch: 2150/10000 Loss (MSE):  train=1.0101338624954224\n",
      "Epoch: 2160/10000 Loss (MSE):  train=1.010043740272522\n",
      "Epoch: 2170/10000 Loss (MSE):  train=1.0099549293518066\n",
      "Epoch: 2180/10000 Loss (MSE):  train=1.0098652839660645\n",
      "Epoch: 2190/10000 Loss (MSE):  train=1.0097764730453491\n",
      "Epoch: 2200/10000 Loss (MSE):  train=1.0096888542175293\n",
      "gammaaaaa:  [[0.99590516]\n",
      " [0.99590516]\n",
      " [0.99590516]\n",
      " [0.99590516]\n",
      " [0.9959053 ]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 2210/10000 Loss (MSE):  train=1.0096007585525513\n",
      "Epoch: 2220/10000 Loss (MSE):  train=1.0095136165618896\n",
      "Epoch: 2230/10000 Loss (MSE):  train=1.0094269514083862\n",
      "Epoch: 2240/10000 Loss (MSE):  train=1.0093398094177246\n",
      "Epoch: 2250/10000 Loss (MSE):  train=1.0092545747756958\n",
      "Epoch: 2260/10000 Loss (MSE):  train=1.009169101715088\n",
      "Epoch: 2270/10000 Loss (MSE):  train=1.0090842247009277\n",
      "Epoch: 2280/10000 Loss (MSE):  train=1.0089998245239258\n",
      "Epoch: 2290/10000 Loss (MSE):  train=1.008915901184082\n",
      "Epoch: 2300/10000 Loss (MSE):  train=1.0088326930999756\n",
      "gammaaaaa:  [[0.99593157]\n",
      " [0.99593157]\n",
      " [0.99593157]\n",
      " [0.99593157]\n",
      " [0.9959316 ]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 2310/10000 Loss (MSE):  train=1.0087498426437378\n",
      "Epoch: 2320/10000 Loss (MSE):  train=1.0086675882339478\n",
      "Epoch: 2330/10000 Loss (MSE):  train=1.0085856914520264\n",
      "Epoch: 2340/10000 Loss (MSE):  train=1.0085039138793945\n",
      "Epoch: 2350/10000 Loss (MSE):  train=1.0084228515625\n",
      "Epoch: 2360/10000 Loss (MSE):  train=1.0083422660827637\n",
      "Epoch: 2370/10000 Loss (MSE):  train=1.008262276649475\n",
      "Epoch: 2380/10000 Loss (MSE):  train=1.0081825256347656\n",
      "Epoch: 2390/10000 Loss (MSE):  train=1.0081031322479248\n",
      "Epoch: 2400/10000 Loss (MSE):  train=1.0080245733261108\n",
      "gammaaaaa:  [[0.9959791]\n",
      " [0.9959791]\n",
      " [0.9959791]\n",
      " [0.9959791]\n",
      " [0.9959791]]\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/loss_array.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/fgnode_trained_model_low.dil ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/training_loss.png ===\n",
      "Epoch: 2410/10000 Loss (MSE):  train=1.0079460144042969\n",
      "Epoch: 2420/10000 Loss (MSE):  train=1.0078682899475098\n",
      "Epoch: 2430/10000 Loss (MSE):  train=1.0077909231185913\n",
      "Epoch: 2440/10000 Loss (MSE):  train=1.0077135562896729\n",
      "Epoch: 2450/10000 Loss (MSE):  train=1.0076375007629395\n",
      "Epoch: 2460/10000 Loss (MSE):  train=1.0075609683990479\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(bRs_in, bRs_out):\n\u001b[1;32m     66\u001b[0m     optimizer_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 67\u001b[0m     opt_state, params, l_ \u001b[39m=\u001b[39m step(\n\u001b[1;32m     68\u001b[0m         optimizer_step, (opt_state, params, \u001b[39m0\u001b[39;49m), \u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m     69\u001b[0m     l \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m l_\n\u001b[1;32m     70\u001b[0m     count\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/GitHub/BGNODE_scratch/.venv_jaxbrow/lib/python3.9/site-packages/jax/example_libraries/optimizers.py:120\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(data, xs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m# The implementation here basically works by flattening pytrees. There are two\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# levels of pytrees to think about: the pytree of params, which we can think of\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# as defining an \"outer pytree\", and a pytree produced by applying init_fun to\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# each leaf of the params pytree, which we can think of as the \"inner pytrees\".\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m# Since pytrees can be flattened, that structure is isomorphic to a list of\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m# lists (with no further nesting).\u001b[39;00m\n\u001b[1;32m    115\u001b[0m OptimizerState \u001b[39m=\u001b[39m namedtuple(\u001b[39m\"\u001b[39m\u001b[39mOptimizerState\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m                             [\u001b[39m\"\u001b[39m\u001b[39mpacked_state\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtree_def\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msubtree_defs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    117\u001b[0m register_pytree_node(\n\u001b[1;32m    118\u001b[0m     OptimizerState,\n\u001b[1;32m    119\u001b[0m     \u001b[39mlambda\u001b[39;00m xs: ((xs\u001b[39m.\u001b[39mpacked_state,), (xs\u001b[39m.\u001b[39mtree_def, xs\u001b[39m.\u001b[39msubtree_defs)),\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mlambda\u001b[39;00m data, xs: OptimizerState(xs[\u001b[39m0\u001b[39m], data[\u001b[39m0\u001b[39m], data[\u001b[39m1\u001b[39m]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m Array \u001b[39m=\u001b[39m Any\n\u001b[1;32m    124\u001b[0m Params \u001b[39m=\u001b[39m Any  \u001b[39m# Parameters are arbitrary nests of `jnp.ndarrays`.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def loss_fn(params, Rs, Rs_1_ac,A=1, B=500): # A=4, B=996 wf=0.996):\n",
    "    Rs_1_pred, gamma = v_v_next_step_pos_gamma_fn(Rs, params)\n",
    "    var =1/gamma\n",
    "    return GaussianNLL(var, Rs_1_pred, Rs_1_ac, A, B)\n",
    "\n",
    "def gloss(*args):\n",
    "    return value_and_grad(loss_fn)(*args)\n",
    "\n",
    "def update(i, opt_state, params, loss__, *data):\n",
    "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "    value, grads_ = gloss(params, *data)\n",
    "    opt_state = opt_update(i, grads_, opt_state)\n",
    "    return opt_state, get_params(opt_state), value\n",
    "\n",
    "@jit\n",
    "def step(i, ps, *args):\n",
    "    return update(i, *ps, *args)\n",
    "\n",
    "opt_init, opt_update_, get_params = optimizers.adam(lr)\n",
    "\n",
    "@jit\n",
    "def opt_update(i, grads_, opt_state):\n",
    "    grads_ = jax.tree_map(jnp.nan_to_num, grads_)\n",
    "    grads_ = jax.tree_map(partial(jnp.clip, a_min=-1000.0, a_max=1000.0), grads_)\n",
    "    return opt_update_(i, grads_, opt_state)\n",
    "\n",
    "def batching(*args, size=None):\n",
    "    L = len(args[0])\n",
    "    if size != None:\n",
    "        nbatches1 = int((L - 0.5) // size) + 1\n",
    "        nbatches2 = max(1, nbatches1 - 1)\n",
    "        size1 = int(L/nbatches1)\n",
    "        size2 = int(L/nbatches2)\n",
    "        if size1*nbatches1 > size2*nbatches2:\n",
    "            size = size1\n",
    "            nbatches = nbatches1\n",
    "        else:\n",
    "            size = size2\n",
    "            nbatches = nbatches2\n",
    "    else:\n",
    "        nbatches = 1\n",
    "        size = L\n",
    "    \n",
    "    newargs = []\n",
    "    for arg in args:\n",
    "        newargs += [jnp.array([arg[i*size:(i+1)*size]\n",
    "                                for i in range(nbatches)])]\n",
    "    return newargs\n",
    "\n",
    "bRs_in, bRs_out = batching(Rs_in, Rs_out, size=min(len(Rs_in), batch_size))\n",
    "\n",
    "print(f\"training ...\")\n",
    "\n",
    "opt_state = opt_init(params)\n",
    "epoch = 0\n",
    "optimizer_step = -1\n",
    "larray = []\n",
    "ltarray = []\n",
    "last_loss = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    l = 0.0\n",
    "    count = 0\n",
    "    for data in zip(bRs_in, bRs_out):\n",
    "        optimizer_step += 1\n",
    "        opt_state, params, l_ = step(\n",
    "            optimizer_step, (opt_state, params, 0), *data)\n",
    "        l += l_\n",
    "        count+=1\n",
    "    # print(\"epoch,countttttt: \", epoch,count)\n",
    "    # opt_state, params, l_ = step(optimizer_step, (opt_state, params, 0), Rs, Vs, Fs)\n",
    "    l = l/count\n",
    "    larray += [l]\n",
    "    # ltarray += [loss_fn(params, bRs_in, bVs_in, bRs_out)]\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}/{epochs} Loss (MSE):  train={larray[-1]}\")#, test={ltarray[-1]}\")\n",
    "    if epoch % 100 == 0:\n",
    "        print('gammaaaaa: ', gamma_fn_model(params))\n",
    "        metadata = {\n",
    "            \"savedat\": epoch,\n",
    "            # \"mpass\": mpass,\n",
    "            }\n",
    "        savefile(f\"fgnode_trained_model.dil\",\n",
    "                    params, metadata=metadata)\n",
    "        # savefile(f\"loss_array.dil\", (larray, ltarray), metadata=metadata)\n",
    "        savefile(f\"loss_array.dil\", larray, metadata=metadata)\n",
    "        if last_loss > larray[-1]:\n",
    "            last_loss = larray[-1]\n",
    "            savefile(f\"fgnode_trained_model_low.dil\",\n",
    "                        params, metadata=metadata)\n",
    "        fig, axs = panel(1, 1)\n",
    "        # plt.semilogy(larray, label=\"Training\")\n",
    "        plt.plot(larray, label=\"Training\")\n",
    "        # plt.semilogy(ltarray, label=\"Test\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(_filename(f\"training_loss.png\"))\n",
    "\n",
    "fig, axs = panel(1, 1)\n",
    "# plt.semilogy(larray, label=\"Training\")\n",
    "plt.plot(larray, label=\"Training\")\n",
    "# plt.semilogy(ltarray, label=\"Test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(_filename(f\"training_loss.png\"))\n",
    "\n",
    "params = get_params(opt_state)\n",
    "savefile(f\"fgnode_trained_model.dil\", params, metadata=metadata)\n",
    "# savefile(f\"loss_array.dil\", (larray, ltarray), metadata=metadata)\n",
    "\n",
    "if last_loss > larray[-1]:\n",
    "    last_loss = larray[-1]\n",
    "    savefile(f\"fgnode_trained_model_low.dil\", params, metadata=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params, _ = loadfile(f\"fgnode_trained_model_low.dil\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rname=False\n",
    "\n",
    "# PSYS = f\"a-{N}-Spring-data-brownian_EM\"\n",
    "# TAG = f\"2BNN\"\n",
    "# out_dir = f\"../results\"\n",
    "\n",
    "def _filename(name, tag=TAG):\n",
    "    rstring = randfilename if (rname and (tag != \"data\")) else (\n",
    "        \"0\" if (tag == \"data\") or (withdata == None) else f\"0_{withdata}\")\n",
    "    filename_prefix = f\"{out_dir}/{PSYS}-{tag}/{rstring}/\"\n",
    "    file = f\"{filename_prefix}/{name}\"\n",
    "    os.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "    filename = f\"{filename_prefix}/{name}\".replace(\"//\", \"/\")\n",
    "    print(\"===\", filename, \"===\")\n",
    "    return filename\n",
    "\n",
    "def OUT(f):\n",
    "    @wraps(f)\n",
    "    def func(file, *args, tag=TAG, **kwargs):\n",
    "        return f(_filename(file, tag=tag), *args, **kwargs)\n",
    "    return func\n",
    "\n",
    "loadmodel = OUT(src.models.loadmodel)\n",
    "savemodel = OUT(src.models.savemodel)\n",
    "\n",
    "loadfile = OUT(src.io.loadfile)\n",
    "savefile = OUT(src.io.savefile)\n",
    "save_ovito = OUT(src.io.save_ovito)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ../results/a-5-Spring-data-brownian_EM-2BNN/0/fgnode_trained_model_low.dil ===\n",
      "Loading ../results/a-5-Spring-data-brownian_EM-2BNN/0/fgnode_trained_model_low.dil\n"
     ]
    }
   ],
   "source": [
    "params, _ = loadfile(f\"fgnode_trained_model_low.dil\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1.0012765],\n",
       "       [1.0012765],\n",
       "       [1.0012765],\n",
       "       [1.0012765],\n",
       "       [1.0012765]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_fn_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "spring_constant = 1.0\n",
    "length_constant = 1.0\n",
    "gamma_orig = jnp.ones(jnp.unique(species).shape)\n",
    "stride = 1\n",
    "runs=100\n",
    "\n",
    "def SPRING(x, stiffness=1.0, length=1.0):\n",
    "    x_ = jnp.linalg.norm(x, keepdims=True)\n",
    "    return 0.5*stiffness*(x_ - length)**4\n",
    "\n",
    "def pot_energy_orig(x):\n",
    "    dr = x[senders, :] - x[receivers, :]\n",
    "    return vmap(partial(SPRING, stiffness=spring_constant, length=length_constant))(dr).sum()\n",
    "\n",
    "def force_fn_orig(R, params):\n",
    "    return -grad(pot_energy_orig)(R)\n",
    "\n",
    "\n",
    "def get_forward_sim(params = None, force_fn = None, gamma = None, runs=10):\n",
    "        @jit\n",
    "        def fn(R,key):\n",
    "            return predition_brow(R, params, force_fn, shift, dt, kT, masses, gamma = gamma, stride=stride, runs=runs, key=key)\n",
    "        return fn\n",
    "\n",
    "gamma_model = gamma_fn_model(params)\n",
    "\n",
    "sim_orig = get_forward_sim(params=None,force_fn=force_fn_orig, gamma=gamma_orig,runs=runs)\n",
    "sim_model = get_forward_sim(params=params,force_fn=force_fn_model, gamma=gamma_model,runs=runs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating trajectory 0/100 ...\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/actual_0_0.xyz ===\n",
      "Saving ovito file: ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/actual_0_0.xyz\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/pred_0_0.xyz ===\n",
      "Saving ovito file: ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/pred_0_0.xyz\n",
      "Simulating trajectory 1/100 ...\n",
      "Simulating trajectory 2/100 ...\n",
      "Simulating trajectory 3/100 ...\n",
      "Simulating trajectory 4/100 ...\n",
      "Simulating trajectory 5/100 ...\n",
      "Simulating trajectory 6/100 ...\n",
      "Simulating trajectory 7/100 ...\n",
      "Simulating trajectory 8/100 ...\n",
      "Simulating trajectory 9/100 ...\n",
      "Simulating trajectory 10/100 ...\n",
      "Simulating trajectory 11/100 ...\n",
      "Simulating trajectory 12/100 ...\n",
      "Simulating trajectory 13/100 ...\n",
      "Simulating trajectory 14/100 ...\n",
      "Simulating trajectory 15/100 ...\n",
      "Simulating trajectory 16/100 ...\n",
      "Simulating trajectory 17/100 ...\n",
      "Simulating trajectory 18/100 ...\n",
      "Simulating trajectory 19/100 ...\n",
      "Simulating trajectory 20/100 ...\n",
      "Simulating trajectory 21/100 ...\n",
      "Simulating trajectory 22/100 ...\n",
      "Simulating trajectory 23/100 ...\n",
      "Simulating trajectory 24/100 ...\n",
      "Simulating trajectory 25/100 ...\n",
      "Simulating trajectory 26/100 ...\n",
      "Simulating trajectory 27/100 ...\n",
      "Simulating trajectory 28/100 ...\n",
      "Simulating trajectory 29/100 ...\n",
      "Simulating trajectory 30/100 ...\n",
      "Simulating trajectory 31/100 ...\n",
      "Simulating trajectory 32/100 ...\n",
      "Simulating trajectory 33/100 ...\n",
      "Simulating trajectory 34/100 ...\n",
      "Simulating trajectory 35/100 ...\n",
      "Simulating trajectory 36/100 ...\n",
      "Simulating trajectory 37/100 ...\n",
      "Simulating trajectory 38/100 ...\n",
      "Simulating trajectory 39/100 ...\n",
      "Simulating trajectory 40/100 ...\n",
      "Simulating trajectory 41/100 ...\n",
      "Simulating trajectory 42/100 ...\n",
      "Simulating trajectory 43/100 ...\n",
      "Simulating trajectory 44/100 ...\n",
      "Simulating trajectory 45/100 ...\n",
      "Simulating trajectory 46/100 ...\n",
      "Simulating trajectory 47/100 ...\n",
      "Simulating trajectory 48/100 ...\n",
      "Simulating trajectory 49/100 ...\n",
      "Simulating trajectory 50/100 ...\n",
      "Simulating trajectory 51/100 ...\n",
      "Simulating trajectory 52/100 ...\n",
      "Simulating trajectory 53/100 ...\n",
      "Simulating trajectory 54/100 ...\n",
      "Simulating trajectory 55/100 ...\n",
      "Simulating trajectory 56/100 ...\n",
      "Simulating trajectory 57/100 ...\n",
      "Simulating trajectory 58/100 ...\n",
      "Simulating trajectory 59/100 ...\n",
      "Simulating trajectory 60/100 ...\n",
      "Simulating trajectory 61/100 ...\n",
      "Simulating trajectory 62/100 ...\n",
      "Simulating trajectory 63/100 ...\n",
      "Simulating trajectory 64/100 ...\n",
      "Simulating trajectory 65/100 ...\n",
      "Simulating trajectory 66/100 ...\n",
      "Simulating trajectory 67/100 ...\n",
      "Simulating trajectory 68/100 ...\n",
      "Simulating trajectory 69/100 ...\n",
      "Simulating trajectory 70/100 ...\n",
      "Simulating trajectory 71/100 ...\n",
      "Simulating trajectory 72/100 ...\n",
      "Simulating trajectory 73/100 ...\n",
      "Simulating trajectory 74/100 ...\n",
      "Simulating trajectory 75/100 ...\n",
      "Simulating trajectory 76/100 ...\n",
      "Simulating trajectory 77/100 ...\n",
      "Simulating trajectory 78/100 ...\n",
      "Simulating trajectory 79/100 ...\n",
      "Simulating trajectory 80/100 ...\n",
      "Simulating trajectory 81/100 ...\n",
      "Simulating trajectory 82/100 ...\n",
      "Simulating trajectory 83/100 ...\n",
      "Simulating trajectory 84/100 ...\n",
      "Simulating trajectory 85/100 ...\n",
      "Simulating trajectory 86/100 ...\n",
      "Simulating trajectory 87/100 ...\n",
      "Simulating trajectory 88/100 ...\n",
      "Simulating trajectory 89/100 ...\n",
      "Simulating trajectory 90/100 ...\n",
      "Simulating trajectory 91/100 ...\n",
      "Simulating trajectory 92/100 ...\n",
      "Simulating trajectory 93/100 ...\n",
      "Simulating trajectory 94/100 ...\n",
      "Simulating trajectory 95/100 ...\n",
      "Simulating trajectory 96/100 ...\n",
      "Simulating trajectory 97/100 ...\n",
      "Simulating trajectory 98/100 ...\n",
      "Simulating trajectory 99/100 ...\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/error_paramete_plot_a_b_c.pkl ===\n",
      "=== ../results/a-5-non-linear-Spring-data-brownian_EM-2BNN/05-24-2023_22-12-17/error_parameter.pkl ===\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "plotthings = True\n",
    "rng_key = random.PRNGKey(0)\n",
    "maxtraj = 100\n",
    "\n",
    "_gamma = gamma_fn_model(params)\n",
    "nexp = {\n",
    "        \"dz_actual\": [],\n",
    "        \"dz_pred\": [],\n",
    "        \"z_actual\": [],\n",
    "        \"z_pred\": [],\n",
    "        \"_gamma\": [_gamma],\n",
    "        \"simulation_time\":[],\n",
    "        }\n",
    "\n",
    "trajectories = []\n",
    "for ind in range(maxtraj):\n",
    "    print(f\"Simulating trajectory {ind}/{maxtraj} ...\")\n",
    "    R, _ = chain(N)[:2]\n",
    "    for rand in range(10):\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "        actual_traj = sim_orig(R,(ind+13)*subkey)\n",
    "        rng_key, subkey = random.split(rng_key)\n",
    "        \n",
    "        start = time.time()\n",
    "        pred_traj = sim_model(R, (ind+13)*subkey)\n",
    "        end = time.time()\n",
    "        nexp[\"simulation_time\"] += [end-start]\n",
    "        \n",
    "        \n",
    "        \n",
    "        nexp[\"dz_actual\"] += [actual_traj.position-R]\n",
    "        nexp[\"dz_pred\"] += [pred_traj.position-R]\n",
    "        \n",
    "        nexp[\"z_actual\"] += [actual_traj.position]\n",
    "        nexp[\"z_pred\"] += [pred_traj.position]\n",
    "        \n",
    "        if save_ovito:\n",
    "            if ind<1 and rand<1:\n",
    "                save_ovito(f\"actual_{ind}_{rand}.xyz\", [state for state in BrownianStates(actual_traj)], lattice=\"\")\n",
    "                save_ovito(f\"pred_{ind}_{rand}.xyz\", [state for state in BrownianStates(pred_traj)], lattice=\"\")\n",
    "        \n",
    "        # trajectories += [(actual_traj, pred_traj)]\n",
    "        # if ind%10==0:\n",
    "            # savefile(\"trajectories.pkl\", trajectories)\n",
    "\n",
    "def KL_divergence(sigma0,mu0,sigma1,mu1, eps=1e-8):\n",
    "    return jnp.log(sigma1/sigma0) + (jnp.square(sigma0)+jnp.square(mu0-mu1))/(2*jnp.square(sigma1)) - 0.5\n",
    "\n",
    "def get_kld(d_actual, d_pred):\n",
    "    mu0 = jnp.mean(d_actual, axis=(0,2,3))\n",
    "    std0 = jnp.std(d_actual, axis=(0,2,3))\n",
    "    mu1 = jnp.mean(d_pred, axis=(0,2,3))\n",
    "    std1 = jnp.std(d_pred, axis=(0,2,3))\n",
    "    kld = []\n",
    "    for i in range(len(std0)):\n",
    "        kld.append(KL_divergence(std0[i],mu0[i],std1[i],mu1[i]))\n",
    "    return jnp.array(kld)\n",
    "\n",
    "def get_std_rmse(d_actual, d_pred):\n",
    "    std0 = jnp.std(d_actual, axis=(0,2,3))\n",
    "    std1 = jnp.std(d_pred, axis=(0,2,3))\n",
    "    return jnp.sqrt(jnp.square(std0 - std1))\n",
    "\n",
    "def get_dist_by_var(actual, pred, zeta):\n",
    "    disp = displacement(actual, pred)\n",
    "    dist_matrix = jnp.sqrt(jnp.square(disp).sum(-1))\n",
    "    dist_mean = jnp.mean(dist_matrix, axis=(0,2))\n",
    "    dist_by_zeta = dist_mean/zeta\n",
    "    return dist_by_zeta\n",
    "\n",
    "nexp2 = {\n",
    "        \"kld\": [],\n",
    "        \"std_rmse\": [],\n",
    "        \"dist_by_var\": [],\n",
    "        }\n",
    "\n",
    "nexp2[\"kld\"] = jnp.array(get_kld(jnp.array(nexp[\"dz_actual\"]),jnp.array(nexp[\"dz_pred\"])))\n",
    "\n",
    "nexp2[\"std_rmse\"] = jnp.array(get_std_rmse(jnp.array(nexp[\"dz_actual\"]),jnp.array(nexp[\"dz_pred\"])))\n",
    "\n",
    "nexp2[\"dist_by_var\"] = jnp.array(get_dist_by_var(jnp.array(nexp['z_actual']), jnp.array(nexp['z_pred']),1/(jnp.array(nexp['_gamma'])[0][0])))\n",
    "\n",
    "savefile(f\"error_paramete_plot_a_b_c.pkl\", nexp2)\n",
    "\n",
    "savefile(f\"error_parameter.pkl\", nexp)\n",
    "# savefile(\"trajectories.pkl\", trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_jaxbrow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
